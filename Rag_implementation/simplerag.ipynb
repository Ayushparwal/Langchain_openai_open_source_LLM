{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rag implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is rag tutorial...\n"
     ]
    }
   ],
   "source": [
    "print(\"this is RAG tutorial...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load steps 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('earth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'earth.txt'}, page_content='Earth, the third planet from the Sun, is a remarkable celestial body teeming with life. Unique in our solar system, it is often called the \"Blue Planet\" due to the abundant water covering about 71% of its surface. Earthâ€™s complex system of land, water, air, and life has allowed for the development of a rich biosphere, making it distinct from other planets.\\n\\nThe planet is about 4.5 billion years old and has a layered structure, consisting of the crust, mantle, outer core, and inner core. The crust is Earth\\'s outermost layer, where we live, and itâ€™s broken into large pieces called tectonic plates. These plates move slowly but are responsible for many geological phenomena like earthquakes, volcanic eruptions, and the formation of mountains. Below the crust lies the mantle, a thick layer of semi-molten rock, which drives plate tectonics through convection currents. At the core, primarily composed of iron and nickel, Earth\\'s inner and outer cores generate a magnetic field that protects the planet from harmful solar radiation and cosmic rays.\\n\\nWater plays a vital role in shaping the planet\\'s surface and sustaining life. Earth\\'s oceans, rivers, lakes, and glaciers are part of the hydrosphere, a system that regulates the climate and supports ecosystems. The water cycleâ€”where water evaporates, condenses into clouds, and falls as precipitationâ€”constantly renews the planetâ€™s freshwater sources.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "\n",
    "                     )))\n",
    "\n",
    "text_documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('encoder-decoder.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'encoder-decoder.pdf', 'page': 0}, page_content='Understanding How Encoder-Decoder\\nArchitectures Attend\\nKyle Aitken\\nDepartment of Physics\\nUniversity of Washington\\nSeattle, Washington, USA\\nkaitken17@gmail.comVinay V Ramasesh\\nGoogle Research, Blueshift Team\\nMountain View, California, USA\\nYuan Cao\\nGoogle Research, Brain Team\\nMountain View, California, USANiru Maheswaranathan\\nGoogle Research, Brain Team\\nMountain View, California, USA\\nAbstract\\nEncoder-decoder networks with attention have proven to be a powerful way to solve\\nmany sequence-to-sequence tasks. In these networks, attention aligns encoder and\\ndecoder states and is often used for visualizing network behavior. However, the\\nmechanisms used by networks to generate appropriate attention matrices are still\\nmysterious. Moreover, how these mechanisms vary depending on the particular\\narchitecture used for the encoder and decoder (recurrent, feed-forward, etc.) are also\\nnot well understood. In this work, we investigate how encoder-decoder networks\\nsolve different sequence-to-sequence tasks. We introduce a way of decomposing\\nhidden states over a sequence into temporal (independent of input) and input-\\ndriven (independent of sequence position) components. This reveals how attention\\nmatrices are formed: depending on the task requirements, networks rely more\\nheavily on either the temporal or input-driven components. These ﬁndings hold\\nacross both recurrent and feed-forward architectures despite their differences in\\nforming the temporal components. Overall, our results provide new insight into the\\ninner workings of attention-based encoder-decoder networks.\\n1 Introduction\\nModern machine learning encoder-decoder architectures can achieve strong performance on sequence-\\nto-sequence tasks such as machine translation (Bahdanau et al., 2014; Luong et al., 2015; Wu et al.,\\n2016; Vaswani et al., 2017), language modeling (Raffel et al., 2020), speech-to-text (Chan et al.,\\n2015; Prabhavalkar et al., 2017; Chiu et al., 2018), etc. Many of these architectures make use of\\nattention (Bahdanau et al., 2014), a mechanism that allows the network to focus on a speciﬁc part of\\nthe input most relevant to the current prediction step. Attention has proven to be a critical mechanism;\\nindeed many modern architectures, such as the Transformer, are fully attention-based (Vaswani et al.,\\n2017). However, despite the success of these architectures, an understanding of how said networks\\nsolve such tasks using attention remains largely unknown.\\nAttention mechanisms are attractive because they are interpretable, and often illuminate key com-\\nputations required for a task. For example, consider neural machine translation—trained networks\\nexhibit attention matrices that align words in the encoder sequence with the appropriate correspond-\\ning position in the decoder sentence (Ghader & Monz, 2017; Ding et al., 2019). In this case, the\\nattention matrix already contains information about which words in the source sequence are relevant\\nfor translating a particular word in the target sequence; that is, forming the attention matrix itself\\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).arXiv:2110.15253v1  [cs.LG]  28 Oct 2021'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 1}, page_content='constitutes a signiﬁcant part of solving the overall task. How is it that networks are able to achieve\\nthis? What are the mechanisms underlying how networks form attention, and how do they vary across\\ntasks and architectures?\\nIn this work, we study these questions by analyzing three different encoder-decoder architectures on\\nsequence-to-sequence tasks. We develop a method for decomposing the hidden states of the network\\ninto a sum of components that let us isolate input driven behavior from temporal (or sequence) driven\\nbehavior. We use this to ﬁrst understand how networks solve tasks where all samples use the same\\nattention matrix, a diagonal one. We then build on that to show how additional mechanisms can\\ngenerate sample-dependent attention matrices that are still close to the average matrix.\\nOur Contributions\\n•We propose a decomposition of hidden state dynamics into separate pieces, one of which explains\\nthe temporal behavior of the network, another of which describes the input behavior. We show\\nsuch a decomposition aids in understanding the behavior of networks with attention.\\n•In the tasks studied, we show the temporal (input) components play a larger role in determining\\nthe attention matrix as the average attention matrix becomes a better (worse) approximation for a\\nrandom sample’s attention matrix.\\n•We discuss the dynamics of architectures with attention and/or recurrence and show how the\\ninput/temporal component behavior differs across said architectures.\\n•We investigate the detailed temporal and input component dynamics in a synthetic setting to\\nunderstand the mechanism behind common sequence-to-sequence structures and how they might\\ndiffer in the presence of recurrence.\\nRelated Work As mentioned in the introduction, a common technique to gain some understanding\\nis to visualize learned attention matrices, though the degree to which such visualization can explain\\nmodel predictions is disputed Wiegreffe & Pinter (2019); Jain & Wallace (2019); Serrano & Smith\\n(2019). Input saliency Bastings & Filippova (2020) and attribution-propagation Chefer et al. (2020)\\nmethods have also been studied as potential tools for model interpretability.\\nComplementary to these works, our approach builds on a recent line of work analyzing the computa-\\ntional mechanisms learned by RNNs from a dynamical systems perspective. These analyses have iden-\\ntiﬁed simple and interpretable hidden state dynamics underlying RNN operation on text-classiﬁcation\\ntasks such as binary sentiment analysis (Maheswaranathan et al., 2019; Maheswaranathan & Sussillo,\\n2020) and document classiﬁcation (Aitken et al., 2020). Our work extends these ideas into the domain\\nof sequence-to-sequence tasks.\\nNotation LetTandSbe the input and output sequence length of a given sample, respectively.\\nWe denote the encoder and decoder hidden states by hE\\nt∈Rnwitht= 1,...,T . Similarly, we\\ndenote decoder hidden states by hD\\ns∈Rn, withs= 1,...,S . The encoder and decoder hidden state\\ndimensions are always taken to be equal in this work. Inputs to the encoder and decoder are denoted\\nbyxE\\nt∈RdandxD\\ns∈R˜d. When necessary, we subscript different samples from a test/train set using\\nα,β,γ , e.g.xE\\nt,αforα= 1,...,M .\\nOutline We begin by introducing the three architectures we investigate in this work with varying\\ncombinations of recursion and attention. Next we introduce our temporal and input component\\ndecomposition and follow this up with a demonstration of how such a decomposition allows us to\\nunderstand the dynamics of attention in a simple one-to-one translation task. Afterwards, we apply\\nthis decomposition to two additional tasks with increasing levels of complexity and discuss how our\\ndecomposition gives insight into the behavior of attention in these tasks.\\n2 Setup\\nA schematic of the three architectures we study is shown in Fig. 1 (see Appendix A.1 for precise\\nexpressions).\\nVanilla Encoder Decoder (VED) is a recurrent encoder-decoder architecture with no attention\\n(Sutskever et al., 2014). The encoder and decoder update expression are hE\\nt=FE(hE\\nt−1,xE\\nt)and\\nhD\\ns=FD(hD\\ns−1,xD\\ns), respectively. Here, FDandFEare functions that implement the hidden\\n2'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 2}, page_content='Figure 1: Schematic of the three primary architectures analyzed in this work. The orange, purple, and\\ngreen boxes represent the encoder RNNs, decoder RNNs, and linear readout layers, respectively. Recurrent\\nconnections are shown in blue, attention-based connections and computational blocks are shown in gold. The\\ngrey circles add positional encoding to the inputs.\\nstate updates, which in this work are each one of three modern RNN cells: LSTMs (Hochreiter &\\nSchmidhuber, 1997), GRUs (Cho et al., 2014), or UGRNNs (Collins et al., 2016).\\nEncoder-Decoder with Attention (AED) is identical to the VED architecture above with a simple\\nattention mechanism added (Bahdanau et al., 2014; Luong et al., 2015). For time step sof the decoder,\\nwe compute a context vector cs, a weighted sum of encoder hidden states, cs:=∑T\\nt=1αsthE\\nt, with\\nαt:=softmax (a1t,...,aSt)thetthcolumn of the attention matrix andast:=hD\\ns·hE\\ntthealignment\\nbetween a given decoder and encoder hidden state. While more complicated attention mechanisms\\nexist, in the main text we analyze the simplest form of attention for convenience of analysis.1\\nAttention Only (AO) is identical to the AED network above, but simply eliminates the recurrent\\ninformation passed from one RNN cell to the next and instead adds ﬁxed positional encoding vectors\\nto the encoder and decoder inputs (Vaswani et al., 2017). Due to the lack of recurrence, the RNN\\nfunctionsFEandFDsimply act as feedforward networks in this setting.2AO can be treated as a\\nsimpliﬁed version of a Transformer without self-attention, hence our analysis may also provide a hint\\ninto their inner workings (Vaswani et al., 2017).\\n2.1 Temporal and Input Components\\nIn architectures with attention, we will show that it is helpful to write the hidden states using what we\\nwill refer to as their temporal andinput components. This will be useful because each hidden state\\nhas an associated time step and input word at that same time step (e.g. sandxD\\nsforhD\\ns), therefore\\nsuch a decomposition will often allow us to disentangle temporal and input behavior from any other\\nnetwork dynamics.\\nWe deﬁne the temporal components of the encoder and decoder to be the average hidden state at a\\ngiven time step, which we denote by µE\\ntandµD\\ns, respectively. Similarly, we deﬁne an encoder input\\ncomponent to be the average of all hE\\nt−µE\\ntfor hidden states that immediately follow a given input\\nword. We analogously deﬁne the decoder input components. In practice, we estimate such averages\\nusing a test set of size M, so that the temporal and input components of the encoder are respectively\\ngiven by\\nµE\\nt≈∑M\\nα=11≤EoS,αhE\\nt,α∑M\\nβ=11≤EoS,β, χE(xt,α)≈∑M\\nβ=1∑T\\nt′=11xt,α,xt′,β(\\nhE\\nt′,β−µE\\nt′)\\n∑M\\nγ=1∑T\\nt′′=11xt,α,xt′′,γ,(1)\\nwhere hE\\nt,αthe encoder hidden state of the αth sample, 1≤EoS,αis a mask that is zero if the αth\\nsample is beyond the end of sentence, 1xt,α,xt′,βis a mask that is zero if xt,α̸=xt′,β, and we\\n1In Appendix B.3, we implement a learned-attention mechanism using a scaled-dot product attention in the\\nform of queries, keys, and value matrices (Vaswani et al., 2017). For the AED and AO architectures, we ﬁnd\\nqualitatively similar results to the simple dot-product attention presented in the main text.\\n2We train non-gated feedforward networks and ﬁnd their dynamics to be qualitatively the same, see Appendix\\nB.\\n3'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 3}, page_content='Figure 2: Summary of attention dynamics on synthetic tasks. (a-f) All three architectures trained on an\\nN= 3one-to-one translation task of variable length ranging from 15to20. Plots in the top row are projected\\nonto the principal components (PCs) of the encoder and decoder temporal components, while those in the\\nbottom row are projected onto the PCs of the input components. (a)For AED, the path formed by the temporal\\ncomponents of the encoder (orange) and decoder (purple), µE\\ntandµD\\ns. We denote the ﬁrst and last temporal\\ncomponent by a square and star, respectively, and the color of said path is lighter for earlier times. The inset\\nshows the softmaxed alignment scores for µD\\ns·µE\\nt, which we ﬁnd to be a good approximation to the full\\nalignment for the one-to-one translation task. (b)The input-delta components of the encoder (light) and decoder\\n(dark) colored by word (see labels). The encoder input components, χE\\nxare represented by a dark colored ‘X’.\\nThe solid lines are the readout vectors (see labels on (d)). Start/end of sentence characters are in purple. (c, d)\\nThe same plots for the AO network. (e, f) The same plots for the VED network (with no attention inset). (g)\\nTemporal components for the same task with a temporally reversed output sequence. (h)Attention matrices\\nfor a test example from a network trained to alphabetically sort a list of letters. Clockwise from top left, the\\nsoftmaxed attention from the full hidden states ( hD\\ns·hE\\nt), temporal components only ( µD\\ns·µE\\nt), decoder input\\ncomponents and encoder delta components ( χD\\ny·∆hE\\nt), and decoder delta components and encoder input\\ncomponents ( ∆hD\\ns·χE\\nx).\\nhave temporarily suppressed superscripts on the inputs for brevity.3By deﬁnition, the temporal\\ncomponents only vary with time and the input components only vary with input/output word . As such,\\nit will be useful to denote the encoder and decoder input components by χE\\nxandχD\\ny, withxandy\\nrespectively running over all input and output words (e.g. χE\\nyesandχD\\noui). We can then write any\\nhidden state as\\nhE\\nt=µE\\nt+χE\\nx+ ∆hE\\nt,hD\\ns=µD\\ns+χD\\ny+ ∆hD\\ns, (2)\\nwith ∆hE\\nt:=hE\\nt−µE\\nt−χE\\ntand∆hD\\ns:=hD\\ns−µD\\ns−χD\\nythedelta components of encoder and\\ndecoder hidden states, respectively. Intuitively, we are simply decomposing each hidden state vector\\nas a sum of a component that only varies with time/position in the sequence (independent of input),\\na component that only varies with input (independent of position), and whatever else is left over.\\nFinally, we will often refer to hidden states without their temporal component, i.e. χE\\nx+ ∆hE\\ntand\\nχD\\ny+ ∆hD\\ns, so for brevity we refer to these combinations as the input-delta components .\\nUsing the temporal and input components in (2), we can decompose the attention alignment between\\ntwo hidden states as\\nast=(\\nµD\\ns+χD\\ny+ ∆hD\\ns)\\n·(\\nµE\\nt+χE\\nx+ ∆hE\\nt)\\n. (3)\\nWe will show below that in certain cases several of the nine terms of this expression approximately\\nvanish, leading to simple and interpretable attention mechanisms.\\n3 One-to-One Results\\nTo ﬁrst establish a basis of how each of the three architectures learn to solve tasks and the role of\\ntheir input and temporal components, we start by studying their dynamics for a synthetic one-to-one\\n3See Appendix A.2 for more details on this deﬁnition and the analogous decoder deﬁnitions.\\n4'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 4}, page_content='translation task. The task is to convert a sequence of input words into a corresponding sequence of\\noutput words, where there is a one-to-one translation dictionary, e.g. converting a sequence of letters\\nto their corresponding position in the alphabet, {B,A,C,A,D}→{ 2,1,3,1,4}. We generate the\\ninput phrases to have variable length, but outputs always have equal length to their input (i.e. T=S).\\nWhile a solution to this task is trivial, it is not obvious how each neural network architecture will\\nsolve the task. Although this is a severely simpliﬁed approximation to realistic sequence-to-sequence\\ntasks, we will show below that many of the dynamics the AED and AO networks learn on this task\\nare qualitatively present in more complex tasks.\\nEncoder-Decoder with Attention. After training the AED architecture, we apply the decomposi-\\ntion of (2)to the hidden states. Plotting the temporal components of both the encoder and decoder,\\nthey each form an approximate circle that is traversed as their respective inputs are read in (Fig. 2a).4\\nAdditionally, we ﬁnd the encoder and decoder temporal components are closest to alignment when\\ns=t. We also plot the input components of the encoder and decoder together with the encoder\\ninput-delta components, i.e. χE\\nx+ ∆hE\\nt, and the network’s readout vectors (Fig. 2b).5We see for\\nthe encoder hidden states, the input-delta components are clustered close to their respective input\\ncomponents, meaning for this task the delta components are negligible. Also note the decoder\\ninput-delta components are signiﬁcantly smaller in magnitude than the decoder temporal components.\\nTogether, this means we can approximate the encoder and decoder hidden states as hE\\nt≈µE\\nt+χE\\nx\\nandhD\\ns≈µD\\ns, respectively. Finally, note the readout vector for a given output word aligns with the\\ninput components of its translated input word, e.g. the readout for ‘ 1’ aligns with the input component\\nfor ‘A’ (Fig. 2b).6\\nFor the one-to-one translation task, the network learns an approximately diagonal attention matrix,\\nmeaning the decoder at time sprimarily attends to the encoder’s hidden state at t=s. Additionally, we\\nﬁnd the temporal and input-delta components to be close to orthogonal for all time steps, which allows\\nthe network’s attention mechanism to isolate temporal dependence rather than input dependence.\\nSince we can approximate the hidden states as hE\\nt≈µE\\nt+χE\\nxandhD\\ns≈µD\\ns, and the temporal\\nand input components are orthogonal, the alignment in (3)can be written simply as ast≈µD\\ns·µE\\nt.\\nThis means that the fullattention is completely described by the temporal components and thus\\ninput-independent (this will not necessarily be true for other tasks, as we will see later).\\nWith the above results, we can understand how AED solves the one-to-one translation task. After\\nreading a given input, the encoder hidden state is primarily composed of an input and temporal\\ncomponent that are approximately orthogonal to one another, with the input component aligned with\\nthe readout of the translated input word (Fig. 2b). The decoder hidden states are approximately made\\nup of only a temporal component, whose sole job is to align with the corresponding encoder temporal\\ncomponent. Temporal components of the decoder and encoder are closest to alignment for t=s,\\nso the network primarily attends to the encoder state hE\\nt=s. The alignment between encoder input\\ncomponents and readouts yields maximum logit values for the correct translation.\\nAttention Only. Now we turn to AO architecture, which is identical to AED except with the recurrent\\nconnections cut, and positional encoding added to the inputs. We ﬁnd that AO has qualitatively\\nsimilar temporal components that give rise to diagonal attention (Fig. 2c) and the input components\\nalign with the readouts (Fig. 2d). Thus AO solves the task in a similar manner as AED. The only\\ndifference is that the temporal components, driven by RNN dynamics in AED, are now driven purely\\nby the positional encoding in AO.\\nVanilla Encoder-Decoder. After training the VED architecture, we ﬁnd the encoder and decoder\\nhidden states belonging to the same time step form clusters, and said clusters are closest to those\\ncorresponding to adjacent time steps. This yields temporal components that are close to one another\\n4Here and in plots that follow, we plot the various components using principal component analysis (PCA)\\nprojections simply as a convenient visualization tool. Other than observation that in some cases the temporal/input\\ncomponents live in a low-dimensional subspace, none of our quantitative analysis is dependent upon the PCA\\nprojections. For all one-to-one plots, a large percentage ( >90%) of the variance is explained by the ﬁrst 2 or 3\\nPC dimensions.\\n5ForNpossible input words, the encoder input components align with the vetrices of an (N−1)-simplex,\\nwhich is similar to the classiﬁcation behavior observed in Aitken et al. (2020).\\n6Since in AED we pass both the decoder hidden state and the context vector to the readout, each readout\\nvector is twice the hidden state dimension. We plot only the readout weights corresponding to the context vector,\\nsince generally those corresponding to the decoder hidden state are negligible, see Appendix B for more details.\\n5'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 5}, page_content='Figure 3: Summary of dynamics for AED and AO architectures trained on eSCAN. (a) Example attention\\nmatrix for the AED architecture. (b)AED network’s temporal components, with the inset showing the attention\\nmatrix from said temporal components. Once again, encoder and decoder components are orange and purple,\\nrespectively and we are projecting onto the temporal component PCs. (c)AED network’s input-delta components,\\ninput components, and readouts, all colored by their corresponding input/output words (see labels). All quantities\\nprojected onto input component PCs. (d, e, f) The same plots for AO.\\nfor adjacent times, with µE\\nTnext to µD\\n1(Figs. 2e). Since there is no attention in this architecture,\\nthere is no incentive for the network to align temporal components of the encoder and decoder as we\\nsaw in AED and AO.\\nAs recurrence is the only method of transferring information across time steps, encoder and de-\\ncoder hidden states must carry all relevant information from preceding steps. Together, this results\\nin the delta components deviating signiﬁcantly more from their respective input components for\\nVED relative to AED and AO (Fig. 2f). That is, since hidden states must hold the information of\\ninputs/outputs for multiple time steps, we cannot expect them to be well approximated by µE\\nt+χE\\nx\\nbecause, by deﬁnition, it is agnostic to the network’s inputs at any time other than t(and similarly for\\nµE\\ns+χE\\ny). As such, the temporal and input component decomposition gains us little insight into the\\ninner workings of the VED architecture. Additional details of the VED architecture dynamics are\\ndiscussed in Appendix B.6.\\nAdditional Tasks. In this section, we brieﬂy address how two additional synthetic tasks can be\\nunderstood using the temporal and input component decomposition. First, consider a task identical to\\nthe one-to-one task, with the target sequence reversed in time, e.g. {B,A,C,A,D}→{ 4,1,3,1,2}.\\nFor this task, we expect an attention matrix that is anti-diagonal (i.e. it is nonzero for t=S+ 1−s).\\nFor the AED and AO networks trained on this task, we ﬁnd their temporal and input component\\nbehavior to be identical to the original one-to-one task with one exception: instead of the encoder and\\ndecoder temporal components following one another, we ﬁnd one trajectory is ﬂipped in such a way\\nas to yield an anti-diagonal attention matrix (Fig. 2g). That is, the last encoder temporal component\\nis aligned with the ﬁrst decoder temporal component and vice versa.\\nSecond, consider the task of sorting the input alphabetically, e.g. {B,C,A,D}→{ A,B,C,D}.\\nFor this example, we expect the network to learn an input-dependent attention matrix that correctly\\npermutes the input sequence. Since there is no longer a correlation between input and output sequence\\nlocation, the average attention matrix is very different from that of a random sample, and so we expect\\nthe temporal components to insigniﬁcantly contribute to the alignment. Indeed, we ﬁnd µD\\ns·µE\\ntto be\\nnegligible, and instead ∆hD\\ns·χE\\nxdominates the alignment values (Fig. 2h).\\n4 Beyond One-to-One Results\\nIn this section we analyze the dynamics of two tasks that have “close-to-diagonal” attention: (1)\\nwhat we refer to as the extended SCAN dataset and (2) translation between English and French\\nphrases. Since we found temporal/input component decomposition to provide little insight into VED\\ndynamics, our focus in this section will be on only the AED and AO architectures. For both tasks we\\nexplore below, parts of the picture we established on the one-to-one task continues to hold. However,\\n6'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 6}, page_content='Figure 4: Summary of features for AO trained on English to French translation. (a) Sample attention\\nmatrix. (b)The encoder (orange) and decoder (purple) temporal components, with a square and star marking the\\nﬁrst and last time step, respectively. Once again, quantities are projected onto the temporal component PCs. The\\ninset shows the attention matrix from the temporal components, i.e. the softmax of µD\\ns·µE\\nt.(c)The dot product\\nbetween the most common output word readouts and the most common input word input components, χE\\nx.\\nwe will see that in order to succeed at these tasks, both AO and AED must implement additional\\nmechanisms on top of the dynamics we saw for the one-to-one task.\\nExtended SCAN (eSCAN) is a modiﬁed version of the SCAN dataset (Lake & Baroni, 2018), in\\nwhich we randomly concatenate a subset of the phrases to form phrases of length 15to20(see\\nAppendix A.3 for details). The eSCAN tasks is close to one-to-one translation, but is augmented\\nwith several additional rules that modify its structure. For example, a common sequence-to-sequence\\nstructure is that a pair of outputs can swap order relative to their corresponding inputs: the English\\nwords ‘green ﬁeld’ translate to ‘champ vert’ in French (with ‘ﬁeld’ ↔‘champ’ and ‘green’ ↔‘vert’).\\nThis behavior is present in eSCAN: when the input word ‘left’ follows a verb the output command\\nmust ﬁrst turn the respective direction and then perform said action (e.g. ‘run left’ →‘LTURN\\nRUN’).\\nThe AED and AO models both achieve ≥98% word accuracy on eSCAN. Looking at a sample\\nattention matrix of AED, we see consecutive words in the output phrase tend to attend to the same\\nencoder hidden states at the end of subphrases in the input phrase (Fig. 3a). Once again decomposing\\nthe AED network’s hidden states as in (2), we ﬁnd the temporal components of the encoder and\\ndecoder form curves that mirror one another, leading to an approximately diagonal attention matrix\\n(Fig. 3b). The delta components are signiﬁcantly less negligible for this task, as evidence by the fact\\nχE\\nx+ ∆hE\\ntaren’t nearly as clustered around their corresponding input component (Fig. 3c). As we\\nwill verify later, this is a direct result of the network’s use of recurrence, since now hidden states\\ncarry information about subphrases, rather than just individual words.\\nTraining the AO architecture on eSCAN, we also observe non-diagonal attention matrices, but in\\ngeneral their qualitative features differ from those of the AED architecture (Fig. 3d). Focusing on\\nthe subphrase mapping ‘run twice’ →‘RUN RUN’, we see the network learns to attend to the word\\npreceding ‘twice’, since it can no longer rely on recurrence to carry said word’s identity forward.\\nOnce again, the temporal components of the encoder and decoder trace out paths that roughly follow\\none another (Fig. 3e). We see input-delta components cluster around their corresponding input\\ncomponents, indicating the delta components are small (Fig. 3f). Finally, we again see the readouts\\nof particular outputs align well with the input components of their corresponding input word.\\nEnglish to French Translation is another example of a nearly-diagonal task. We train the AED\\nand AO architectures on this natural language task using a subset of the para_crawl dataset Bañón\\net al. (2020) consisting of over 30 million parallel sentences. To aid interpetation, we tokenize each\\nsentence at the word level and maintain a vocabulary of 30k words in each language; we train on\\nsentences of length up to 15 tokens.\\nSince English and French are syntactically similar with roughly consistent word ordering, the attention\\nmatrices are in general close to diagonal (Fig. 4a). Again, note the presence of features that require\\noff-diagonal attention, such as the ﬂipping of word ordering in the input/output phrases and multiple\\nwords in French mapping to a single English word. Using the decomposition of (2), the temporal\\ncomponents in both AED and AO continue to trace out similar curves (Fig. 4b). Notably, the\\nalignment resulting from the temporal components is signiﬁcantly less diagonal, with the diagonal\\nbehavior clearest at the beginning of the phrase. Such behavior makes sense: the presence of off-\\ndiagonal structure means, on average, translation pairs become increasingly offset the further one\\n7'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 7}, page_content='Figure 5: Temporal and input component features. In the ﬁrst three plots, the data shown in red, blue, and\\ngreen corresponds to networks trained on the one-to-one, eSCAN, and English to French translation tasks,\\nrespectively. (a)Breakdown of the nine terms that contribute to the largest alignment scores (see (3)) averaged\\nacross the entire decoder sequence for each task/architecture combination (see Appendix A.2 for details). For\\neach bar, from top to bottom, the alignment contributions from µD\\ns·µE\\nt(dark), µD\\ns·χE\\nx+µD\\ns·∆hE\\nt(medium),\\nand the remaining six terms (light). (b)For the AO architecture, the dot product of the temporal components,\\nµD\\ns·µE\\nt, as a function of the offset, t−s, shown at different decoder times. Each offset is plotted from [−5,5]\\nand the dotted lines show the theoretical prediction for maximum offset as a function of decoder time, s. Plots\\nfor the AED architecture are qualitatively similar. (c)For all hidden states corresponding to an input word, the\\nratio of variance of hE\\nt−µE\\nttohE\\nt.(d)For AO trained on eSCAN, the dot product of input components, χE\\nx,\\nwith each of the readouts (AED is qualitatively similar).\\nmoves into a phrase. With offsets that increasingly vary from phrase to phrase, the network must rely\\nless on temporal component alignments, which by deﬁnition are independent of the inputs. Finally,\\nwe see that the the dot product between the input components and the readout vectors implement\\nthe translation dictionary, just as it did for the one-to-one task (Fig. 4c, see below for additional\\ndiscussion).\\n4.1 A Closer Look at Model Features\\nAs expected, both the AED and AO architectures have more nuanced attention mechanisms when\\ntrained on eSCAN and translation. In this section, we investigate a few of their features in detail.\\nAlignment Approximation. Recall that for the one-to-one task, we found the alignment scores\\ncould be well approximated by ast≈µD\\ns·µE\\nt, which was agnostic to the details of the input\\nsequence. For eSCAN, the µD\\ns·µE\\ntterm is still largely dominant, capturing >77% ofastin\\nthe AED and AO networks (Fig. 5a). A better approximation for the alignment scores is ast≈\\nµD\\ns·µE\\nt+µD\\ns·χE\\nx+µD\\ns·∆hE\\nt, i.e. we include two additional terms on top of what was used for\\none-to-one. Since χE\\nxand∆hE\\ntare dependent upon the input sequence, this means the alignment has\\nnon-trivial input dependence, as we would expect. In both architectures, we ﬁnd this approximation\\ncaptures>87% of the top alignment scores. For translation, we see the term µD\\ns·µE\\ntmakes up a\\nsigniﬁcantly smaller portion of the alignment scores, and in general we ﬁnd none of the nine terms in\\n(3)dominate above the rest (Fig. 5a). However, at early times in the AED architecture, we again see\\nµD\\ns·µE\\ntis the largest contribution to the alignment. As mentioned above, this matches our intuition\\nthat words at the start of the encoder/decoder phrase have a smaller offset from one another than later\\nin the phrase, so the network can rely more on temporal components to determine attention.\\nTemporal Component Offset. For the one-to-one task, the input sequence length was always equal\\nto the output sequence length, so the temporal components were always peaked at s=t(Fig. 5b). In\\neSCAN, the input word ‘and’ has no corresponding output, which has a non-trivial effect on how\\nthe network attends since its appearance means later words in input phrase are offset from their\\ncorresponding output word. This effect also compounds with multiple occurrences of ‘and’ in the\\ninput. The AED and AO networks learn to handle such behavior by biasing the temporal component\\ndot product, µD\\ns·µE\\nt, the dominant alignment contribution, to be larger for time steps tfurther along\\nin the encoder phrase, i.e. t>s (Fig. 5b). It is possible to compute the average offset of input and\\noutput words in eSCAN training set, and we see the maximum of µD\\ns·µE\\ntfollow this estimate quite\\nwell. Similarly, in our set of English to French translation phrases, we ﬁnd the French phrases to be\\non average∼20% longer than their English counterparts. This results in the maximum of µD\\ns·µE\\nt\\nto gradually move toward t<s , e.g. on average the decoder attends to earlier times in the encoder\\n(Fig. 5b). Additionally, note the temporal dot product falls off signiﬁcantly slower as a function of\\noffset for later time steps, indicating the drop off for non-diagonal alignments is smaller and thus it is\\neasier for the network to off-diagonally attend.\\n8'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 8}, page_content='Figure 6: How AO and AED networks implement off-diagonal attention in the eSCAN dataset. (a) For\\nAED, the input-delta components for various words and subphrases. (b)For AO, the alignment values, ast, are\\nshown in black when the input word ‘twice’ is at t=s. Three contributions to the alignment, µD\\ns·µE\\nt(gold),\\nµD\\ns·χE\\nx+µD\\ns·∆hE\\nt(pink), and ast−µD\\ns·hE\\nt(grey) are also plotted. To keep the offset between ‘twice’ and\\nthe output location of the repeated word constant, this plot was generated on a subset of eSCAN with T=S,\\nbut we observe the same qualitative features when T≥S.(c)The dot product between χE\\nx+ ∆hE\\ntand the\\ndecoder’s temporal component, µD\\ns, fort=s.(d)How the dot product of χE\\nx+ ∆hE\\ntandµD\\nschanges as a\\nfunction of their offset, t−s, for a few select input words. The vertical gray slice represents the data in (c) and\\nthe input word colors are the same.\\nWord Variance. The encoder hidden states in the one-to-one task had a negligible delta component,\\nso the hidden states could be approximated as hE\\nt≈µE\\nt+χE\\nx. By deﬁnition, χE\\nxis constant for a\\ngiven input word, so the variance in the hidden states corresponding to a given input word is primarily\\ncontained in the temporal component (Fig. 5c). Since the temporal component is input-independent,\\nthis led to a clear understanding of how all of a network’s hidden states evolve with time and input.\\nIn the AED and AO architectures trained on eSCAN, we ﬁnd the variance of the input word’s hidden\\nstates drops by 90% and95% when the temporal component is subtracted out, respectively (Fig. 5c).\\nMeanwhile, in translation, we ﬁnd the variance only drops by 8%and25% for the AED and AO\\narchitectures, indicating there is signiﬁcant variance in the hidden states beyond the average temporal\\nevolution and thus more intricate dynamics.\\nInput/Readout Alignment. Lastly, recall we saw that in the one-to-one case the input components’\\nalignment with readouts implemented the translation dictionary (Figs. 2b, d). For eSCAN, the dot\\nproduct of a given readout is again largest with the input component of its corresponding input word,\\ne.g. the readout corresponding to ‘RUN’ is maximal for the input component of ‘run’ (Fig. 5d).\\nNotably, words that produce no corresponding output such as ‘and’ and ‘twice’ are not the maximal\\nin alignment with any readout vector. Similarly, for translation, we see the French-word readouts\\nhave the largest dot product their translated English words (Fig. 4c). For example, the readouts for\\nthe words ‘la’, ‘le’, and ‘les’, which are the gendered French equivalents of ‘the’, all have maximal\\nalignments with χE\\nthe.\\n4.2 A Closer Look at Dynamics\\nIn this section, we leverage the temporal and input component decomposition to take a closer look at\\nhow networks trained on the eSCAN dataset implement particular off-diagonal attentions. Many of\\nthe sequence translation structures in eSCAN are seen in realistic datasets, so we this analysis will\\ngive clues toward understanding the behavior of more complicated sequence-to-sequence tasks.\\nA common structure in sequence-to-sequence tasks is when an output word is modiﬁed by the words\\npreceding it. For example, the phrases ‘we run‘ and ‘they run’ translate to ‘nous courrons’ and ‘ils\\ncourent’ in French, respectively (with the second word in each the translation of ‘run’). We can study\\nthis phenomenon in eSCAN since the word ‘twice’ tells the network to repeat the command just\\nissued two times, e.g. ‘run twice’ outputs to ‘RUN RUN’. Hence, the output corresponding to the\\ninput ‘twice’ changes based on other words in the phrase.\\nSince an AED network has recurrence, when it sees the word ‘twice’ it can know what verb preceded\\nit. Plotting input-delta components, we see the RNN outputs ‘twice’ hidden states in three separate\\nclusters separated by the preceding word (Fig. 6a). Thus for an occurrence of ‘twice’ at time step t,\\nwe have χE\\ntwice+ ∆hE\\nt≈χE\\nverb+ ∆hE\\nt−1. For example, this means the AED learns to read in ‘run\\ntwice’ approximately the same as ‘run run’. This is an example of the network learning context.\\n9'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 9}, page_content='AO has no recurrence, so it can’t know which word was output before ‘twice’. Hence, unlike the\\nAED case, all occurrences of ‘twice’ are the same input-delta component cluster regardless of what\\nword preceded it. Instead, it has to rely on attending to the word that modiﬁes the output, which in\\nthis case is simply the preceding word (Fig. 3d). As mentioned in Sec. 4.1, for the eSCAN task we\\nﬁnd the alignment to be well approximated by ast≈µD\\ns·hE\\nt. When the word ’twice’ appears in the\\ninput phrase, we ﬁnd µD\\ns·χE\\ntwice+µD\\ns·∆hE\\nt<0fors=t(Fig. 6b). This decreases the value of\\nthe alignment as,s, and so the decoder instead attends to the time step with the second largest value\\nofµD\\ns·µE\\nt, which the network has learned to be t=s−1. Hence,as,s−1is the largest alignment,\\ncorresponding to the time step before ‘twice’ with the verb the network needs to output again. Unlike\\nthe one-to-one case, the encoder input-delta and the decoder temporal components are no longer\\napproximately orthogonal to one another (Fig. 6c). In the case of ‘twice’, χE\\ntwice+ ∆hE\\ntis partially\\nantialigned with the temporal component, yielding a negative dot product.\\nThis mechanism generalizes beyond the word ’twice’: in eSCAN we see input-delta components of\\nseveral input words are no longer orthogonal to the decoder’s temporal component (Fig. 6c). Like\\n‘twice’, the dot product of the input-delta component for a given word with its corresponding temporal\\ncomponent determines how much its alignment score is increased/decreased. For example, we see\\nχE\\nand+ ∆hE\\nthas a negative dot product with the temporal component, meaning it leans away from its\\ncorresponding temporal component. Again, this make sense from eSCAN task: the word ‘and’ has\\nno corresponding output, hence it never wants to be attended to by the decoder.\\nPerhaps contradictory to expectation, χE\\nleft+ ∆hE\\nthas a negative dot product with the temporal\\ncomponent. However, note that the alignment of χE\\nx+ ∆hE\\ntwith the hD\\nsis dependent on both tands.\\nWe plot the dot products of χE\\nx+ ∆hE\\ntandhD\\nsas a function of their offset, deﬁned to be the t−s\\n(Fig. 6d). Notably, χE\\nleft+ ∆hE\\nthas a larger dot product for larger offsets, meaning it increases its\\nalignment when t>s . This makes sense from the point of view that the word ‘left’ is always further\\nalong in the input phrase than its corresponding output ‘LTURN’, and this offset is only compounded\\nby the presence of the word ‘and’. Thus, the word ‘left’ only wants to get noticed if it is ahead of the\\ncorresponding decoder time step, otherwise it hides. Additionally, the words ‘and‘ and ‘twice‘ have\\nlarge negative dot products for all offsets, since they never want to be the subject of attention.\\n5 Discussion\\nIn this work, we studied the hidden state dynamics of sequence-to-sequence tasks in architectures\\nwith recurrence and attention. We proposed a decomposition of the hidden states into parts that\\nare input- and time-independent and showed when such a decomposition aids in understanding the\\nbehavior of encoder-decoder networks.\\nAlthough we have started by analyzing translation tasks, it would be interesting to understand how said\\ndecomposition works on different sequence-to-sequence tasks, such as speech-to-text. Additionally,\\nwith our focus on the simplest encoder-decoder architectures, it is important to investigate how\\nmuch the observed dynamics generalize to more complicated network setups, such as networks with\\nbidirectional RNNs or multiheaded and self-attention mechanisms. Our analysis of the attention-\\nonly architecture, which bears resemblance to the transformer architecture, suggests that a similar\\ndynamical behavior may also hold for the Transformer, hinting at the working mechanisms behind\\nthis popular non-recurrent architecture.\\nAcknowledgments and Disclosure of Funding\\nWe thank Ankush Garg for collaboration during the early part of this work. None of the authors\\nreceive third-party funding/support during the 36 months prior to this submission or had competing\\ninterests.\\nReferences\\nAitken, K., Ramasesh, V . V ., Garg, A., Cao, Y ., Sussillo, D., and Maheswaranathan, N. The geometry\\nof integration in text classiﬁcation rnns. arXiv preprint arXiv:2010.15114 , 2020.\\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine translation by jointly learning to align and\\ntranslate. arXiv preprint arXiv:1409.0473 , 2014.\\n10'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 10}, page_content='Bañón, M., Chen, P., Haddow, B., Heaﬁeld, K., Hoang, H., Esplà-Gomis, M., Forcada, M. L.,\\nKamran, A., Kirefu, F., Koehn, P., Ortiz Rojas, S., Pla Sempere, L., Ramírez-Sánchez, G.,\\nSarrías, E., Strelec, M., Thompson, B., Waites, W., Wiggins, D., and Zaragoza, J. ParaCrawl:\\nWeb-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the\\nAssociation for Computational Linguistics , pp. 4555–4567, Online, July 2020. Association for\\nComputational Linguistics. doi: 10.18653/v1/2020.acl-main.417. URL https://www.aclweb.\\norg/anthology/2020.acl-main.417 .\\nBastings, J. and Filippova, K. The elephant in the interpretability room: Why use attention as\\nexplanation when we have saliency methods?, 2020.\\nChan, W., Jaitly, N., Le, Q. V ., and Vinyals, O. Listen, attend and spell, 2015.\\nChefer, H., Gur, S., and Wolf, L. Transformer interpretability beyond attention visualization, 2020.\\nChiu, C., Sainath, T. N., Wu, Y ., Prabhavalkar, R., Nguyen, P., Chen, Z., Kannan, A., Weiss,\\nR. J., Rao, K., Gonina, E., Jaitly, N., Li, B., Chorowski, J., and Bacchiani, M. State-of-the-art\\nspeech recognition with sequence-to-sequence models. In 2018 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP) , pp. 4774–4778, 2018. doi: 10.1109/ICASSP.\\n2018.8462105.\\nCho, K., van Merrienboer, B., Gülçehre, Ç., Bougares, F., Schwenk, H., and Bengio, Y . Learning\\nphrase representations using RNN encoder-decoder for statistical machine translation. CoRR ,\\nabs/1406.1078, 2014.\\nCollins, J., Sohl-Dickstein, J., and Sussillo, D. Capacity and trainability in recurrent neural networks,\\n2016.\\nDing, S., Xu, H., and Koehn, P. Saliency-driven word alignment interpretation for neural machine\\ntranslation. arXiv preprint arXiv:1906.10282 , 2019.\\nGhader, H. and Monz, C. What does attention in neural machine translation pay attention to? arXiv\\npreprint arXiv:1710.03348 , 2017.\\nHochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation , 9(8):1735–1780,\\n1997.\\nJain, S. and Wallace, B. C. Attention is not explanation, 2019.\\nKingma, D. and Ba, J. Adam: A method for stochastic optimization. International Conference on\\nLearning Representations , 12 2014.\\nLake, B. and Baroni, M. Generalization without systematicity: On the compositional skills of\\nsequence-to-sequence recurrent networks. In International Conference on Machine Learning , pp.\\n2873–2882. PMLR, 2018.\\nLuong, M.-T., Pham, H., and Manning, C. D. Effective approaches to attention-based neural machine\\ntranslation. arXiv preprint arXiv:1508.04025 , 2015.\\nMaheswaranathan, N. and Sussillo, D. How recurrent networks implement contextual processing in\\nsentiment analysis. arXiv preprint arXiv:2004.08013 , 2020.\\nMaheswaranathan, N., Williams, A., Golub, M., Ganguli, S., and Sussillo, D. Reverse engineering\\nrecurrent networks for sentiment classiﬁcation reveals line attractor dynamics. In Advances in\\nNeural Information Processing Systems 32 , pp. 15696–15705. Curran Associates, Inc., 2019.\\nPrabhavalkar, R., Rao, K., Sainath, T., Li, B., Johnson, L., and Jaitly, N. A comparison of sequence-\\nto-sequence models for speech recognition. 2017. URL http://www.isca-speech.org/\\narchive/Interspeech_2017/pdfs/0233.PDF .\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J.\\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2020.\\nSerrano, S. and Smith, N. A. Is attention interpretable? arXiv preprint arXiv:1906.03731 , 2019.\\n11'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 11}, page_content='Sutskever, I., Vinyals, O., and Le, Q. V . Sequence to sequence learning with neural networks. arXiv\\npreprint arXiv:1409.3215 , 2014.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\\nPolosukhin, I. Attention is all you need. In Advances in neural information processing systems , pp.\\n5998–6008, 2017.\\nWiegreffe, S. and Pinter, Y . Attention is not not explanation, 2019.\\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M., Macherey, W., Krikun, M., Cao, Y ., Gao, Q.,\\nand et al., K. M. Google’s neural machine translation system: Bridging the gap between human\\nand machine translation, 2016.\\nA Additional Details\\nFigure 7: Comparison of the three primary architectures used in this work and the relation between\\nthem. The three architectures are vanilla encoder-decoder (VED), encoder-decoder with attention (AED), and\\nattention only (AO). The encoder RNNs, decoder RNNs, and linear readout layer are showing in orange, purple,\\nand green, respectively. Recurrent connections between RNNs are shown in blue, attention-based connections\\nand computational blocks are shown in gold. For AO, the grey circles represent locations where positional\\nencoding is added to the inputs. Note AED’s linear readout layer takes in both the context vector from attention\\nas well as the decoder’s output.\\nIn this section we provide additional details regarding the architectures, temporal-input component\\ndecomposition, datasets, RNNs, and training used in this work.\\nAs a reminder, the encoder and decoder hidden states are denoted by hE\\ntandhD\\ns, respectively, while\\ninputs to the encoder and decoder are denoted by xE\\ntandxD\\ns.\\nA.1 Architectures\\nA summary of the three architectures we focus on—vanilla encoder-decoder, encoder-decoder with\\nattention, and attention only—in this work is shown in Fig. 7. Intuitively, the architectures are related\\nas follows: AED has attention and recurrence, VED and AO are the same as AED with the attention\\nand recurrence removed, respectively (for AO, we also add positional encoding).\\nVanilla Encoder Decoder\\nThe encoder and decoder update expression are\\nhE\\nt=FE(hE\\nt−1,xE\\nt), hD\\ns=FD(hD\\ns−1,xD\\ns), (4)\\nrespectively. Here, FDandFEare the functions that implement the hidden state updates, which in\\nthis work are each one of three modern RNN architectures: LSTMs (Hochreiter & Schmidhuber,\\n1997), GRUs (Cho et al., 2014), or UGRNNs (Collins et al., 2016). The ﬁnal encoder hidden state is\\nthe decoder’s initial hidden state, so that hD\\n0=hE\\nT. The decoder hidden states are passed through a\\nlinear layer to get the output logits at each time step, ys=WhD\\ns+b, with the following decoder\\ninput, xD\\ns+1, determined by the word corresponding to the maximum output logit, argmax (ys).\\n12'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 12}, page_content='Encoder Decoder with Attention\\nThe encoder-decoder with attention architecture is identical to the VED architecture above with a\\nsimple attention mechanism added (Bahdanau et al., 2014; Luong et al., 2015). For time step sof the\\ndecoder, we compute a context vector cs, a weighted sum of encoder hidden states,\\ncs:=T∑\\nt=1αsthE\\nt, αst:=east\\n∑T\\nt′=1east′. (5)\\nHere, αt:=softmax (a1t,...,aSt)is thet-th column of the attention matrix andast:=hD\\ns·hE\\nt\\nthealignment between a given decoder and encoder hidden state. Furthermore, the outputs are now\\ndetermined by passing both the decoder hidden state and the context vector through the linear layer,\\ni.e.ys=W[hD\\ns,cs] +b, where [·,·]denotes concatenation Luong et al. (2015).\\nAttention Only\\nAttention only is identical to the AED network above, but simply eliminates the recurrent information\\npassed from one RNN cell to the next. Since this eliminates any sense of temporal ordering in the\\nsequences, we also add ﬁxed positional encoding vectors (Vaswani et al., 2017), pE\\ntandpD\\nsto the\\nencoder and decoder inputs. Together, this means the hidden state update expressions are now\\nhE\\nt=FE(0,xE\\nt+pE\\nt), hD\\ns=FD(0,xD\\ns+pD\\ns). (6)\\nNote the RNN functions FEandFDsimply act as feedforward networks in this setting. Lastly, the\\noutput logits are now determined solely from the context vector, ys=Wcs+b.\\nAlthough using gated RNNs cells as feedforward networks is fairly non-standard, our primary\\nmotivation is to keep the AED and AO architectures as similar as possible in order to isolate the\\ndifferences that arise from recurrence and positional encoding. Below we discuss a non-gated\\nfeedforward variant that we also brieﬂy investigate.\\nNote the elimination of recurrence means the entire encoder hidden states sequence can be computed\\nin parallel. This architecture is meant to be a simpliﬁed model of a Transformer (Vaswani et al.,\\n2017). The hidden states output by the RNN simultaneous ﬁll the role of the usual keys, queries, and\\nvalues of a Transformer. Additionally, there is no self-attention mechanism, only a single “head” of\\nattention, and no residual connections.\\nPositional Encoding For ad-dimensional embedding dimension at time t, we add the vector pE\\nt\\nwithi= 0,...,d−1components:\\npE\\nt,i={sin(t\\nτi/d)\\nieven\\ncos(t\\nτ(i−1)/d)\\niodd(7)\\nwithτsome temporal scale that should be related to the phrase length. This is the same positional\\nencoding used in Vaswani et al. (2017), and we use the same encoding for both the encoder ( pE\\nt) and\\ndecoder ( pD\\nt).\\nNon-Gated Variant As mentioned above, we use a gated-RNN with its recurrence cut as a feed-\\nforward network. Since this is fairly non-standard, we also verify some of our results on non-gated\\nfeedforward architectures. The non-gated variant of AO uses a the following hidden-state updates\\nhE\\nt=F′\\nE(xE\\nt) := tanh(\\nWExE\\nt+bE)\\n, (8a)\\nhD\\ns=F′\\nD(xD\\ns) := tanh(\\nWDxD\\ns+bD)\\n, (8b)\\nwith tanh acting pointwise. This architecture is identical to the version of AO used above, but the\\nhidden state updates are now\\nhE\\nt=F′\\nE(xE\\nt+pE\\nt), hD\\ns=F′\\nD(xD\\ns+pD\\ns). (9)\\nBelow, we show that we ﬁnd the qualitative results of this network are the same as the gated\\nversion, and thus our results do not seem to be dependent upon the gating mechanisms present in the\\nfeedforward networks of the AO architecture.\\n13'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 13}, page_content='A.2 Temporal and Input Components\\nWe deﬁne the temporal components to be the average hidden state at a given time step. In practice,\\nwe estimate such averages using a test set of size M, so that the temporal components are given by\\nµE\\nt≈∑M\\nα=11≤EoS,αhE\\nt,α∑M\\nβ=11≤EoS,β, (10a)\\nµD\\ns≈∑M\\nα=11≤EoS,αhD\\ns,α∑M\\nβ=11≤EoS,β, (10b)\\nwithhE\\nt,αthe encoder hidden state of the αth sample and 1≤EoS,αis a mask that is zero if the αth\\nsample is beyond the end of sentence. Next, we deﬁne the encoder input components to be the average\\nofhE\\nt−µE\\ntfor all hidden states that immediately follow a given input word (and similarly for the\\ndecoder input components). Once again, we estimate the input components using a test set of size M,\\nχE(xt,α)≈∑M\\nβ=1∑T\\nt′=11xt,α,xt′,β(\\nhE\\nt′,β−µE\\nt′)\\n∑M\\nγ=1∑T\\nt′′=11xt,α,xt′′,γ, (11a)\\nχD(xs,α)≈∑M\\nβ=1∑S\\ns′=11xs,α,xs′,β(\\nhD\\ns′,β−µD\\ns′)\\n∑M\\nγ=1∑S\\ns′′=11xs,α,xs′′,γ, (11b)\\nwhere 1xt,α,xt′,βis a mask that is zero if xt,α̸=xt′,βand we have temporally suppressed the\\nsuperscripts on the inputs for brevity. With the above deﬁnitions, we can decompose encoder and\\ndecoder hidden states resulting from the αth sample as\\nhE\\nt,α=µE\\nt+χE(\\nxE\\nt,α)\\n+ ∆hE\\nt,α, (12a)\\nhD\\ns,α=µD\\ns+χD(\\nxD\\ns,α)\\n+ ∆hD\\ns,α, (12b)\\nwith the delta components deﬁned to be whatever is leftover in the hidden state after subtracting out\\nthe temporal and input components,\\n∆hE\\nt,α:=hE\\nt,α−µE\\nt−χE(\\nxE\\nt,α)\\n, (13a)\\n∆hD\\ns,α:=hD\\ns,α−µD\\ns−χD(\\nxD\\ns,α)\\n. (13b)\\nIn the main text we use the shorthand χE\\nx=χE(\\nxE\\nt,α)\\nandχD\\ny=χD(\\nxD\\ns,α)\\n(since for the decoder,\\nthe previous time step’s output, ys−1is passed as the next input). We will often suppress the batch\\nindex, so altogether the decomposition is written in the main text as\\nhE\\nt=µE\\nt+χE\\nx+ ∆hE\\nt, (14a)\\nhD\\ns=µD\\ns+χD\\ny+ ∆hD\\ns. (14b)\\nThe intuition behind this decomposition is that it is an attempt to isolate the temporal and input\\nbehavior of the network’s hidden state updates FEandFD. This partially motivated by the fact that, if\\nFEwere linear, then the encoder hidden state update for AO could be written as\\nhE\\nt=FE(0,xE\\nt+pE\\nt) =FE(0,xE\\nt) +FE(0,pE\\nt). (15)\\nNotably, the ﬁrst term is only dependent upon the input and the second term is only dependent upon\\nthe sequence index (through the positional encoding). In this case, the temporal and input component\\nwould exactly capture the time and input dependence of the hidden states, respectively. Of course, FE\\nis not in general linear, but we still ﬁnd such a decomposition useful for interpretation.\\nAlignment Using the above decomposition, we can write the alignment as a sum of nine terms:\\nast=(\\nµD\\ns+χD\\ny+ ∆hD\\ns)\\n·(\\nµE\\nt+χE\\nx+ ∆hE\\nt)\\n=µD\\ns·µE\\nt+µD\\ns·χE\\nx+µD\\ns·∆hE\\nt\\n+χD\\ny·µE\\nt+χD\\ny·χE\\nx+χD\\ny·∆hE\\nt\\n+ ∆hD\\ns·µE\\nt+ ∆hD\\ns·χE\\nx+ ∆hD\\ns·∆hE\\nt (16a)\\n:=9∑\\nI=1a(I)\\nst, (16b)\\n14'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 14}, page_content='Input Phrase Output Phrase\\nrun⟨.⟩jump⟨.⟩walk⟨.⟩RUN⟨.⟩JUMP⟨.⟩WALK⟨.⟩\\nrun left⟨.⟩ LTURN RUN⟨.⟩\\nrun twice⟨.⟩jump⟨.⟩ RUN RUN⟨.⟩JUMP⟨.⟩\\nrun and jump⟨.⟩ RUN JUMP⟨.⟩\\nTable 1: Extended SCAN example phrases.\\nwitha(I)\\nstforI= 1,..., 9deﬁned as the nine terms which sequentially appear after the second\\nequality (i.e. a(1)\\nst:=µD\\ns·µE\\nt,a(2)\\nst:=µD\\ns·χE\\nx, and so on).\\nIn the main text, we measure the breakdown of the alignment scores from each of the nine terms.\\nDeﬁne the contributions from one of the nine terms as\\nA(I)\\nst:=⏐⏐⏐a(I)\\nst⏐⏐⏐\\n∑9\\nJ=1⏐⏐⏐a(J)\\nst⏐⏐⏐, (17)\\nwhere the absolute values are necessary because contributions to the dot product alignment can be\\npositive or negative.\\nA.3 Datasets\\nOne-to-One Dataset This is a simple sequence to sequence task consisting variable length phrases\\nwith input and output words that are in one-to-one correspondence. At each time step, a word from a\\nword bank of size Nis randomly chosen (uniformly), and as such there is no correlation between\\nwords at different time steps. The length of a given input phrase is predetermined and also drawn a\\nuniform distribution. After an input phrase is generated, the corresponding output phrase is created by\\nindividually translating each word. All input words translate to a unique output word and translations\\nare solely dependent upon the input word. An example one-to-one dataset would be converting a\\nsequence of letters to their corresponding position in the alphabet, {B,A,C,A,A}→{ 2,1,3,1,1}.\\nNote the task of simply repeating the input phrase as the output is also one-to-one.\\nDue to the small vocabulary size, one-hot encoding is used for input phrases. Train and test sets are\\ngenerated dynamically.\\nExtended SCAN Extended SCAN (eSCAN) is a modiﬁed version of the SCAN dataset Lake &\\nBaroni (2018). The SCAN dataset is a sequence-to-sequence task consisting of translating simple\\ninput commands into output actions. SCAN consists of roughly 20,000 phrases, with a maximum\\ninput and output phrase lengths of 9 and 49, respectively. A few relevant example phrases of eSCAN\\nare shown in Table 1.\\nThe eSCAN dataset modiﬁes SCAN in two ways:\\n1.It takes only a subset of SCAN in which input phrases solely consist of a chosen subset\\nof SCAN words. This allows us to isolate particular behaviors present in SCAN as well\\nas eliminate certain word combinations that would require far-from diagonal attention\\nmechanisms (e.g. it allows us to avoid the input subphrase ‘run around thrice’ that yields an\\noutput subphrase of length 24).\\n2.After a subset of SCAN phrases has been chosen, phrases are randomly drawn (uniformly)\\nand concatenated together until a phrase of the desired length range is created. Individual\\nSCAN phrases are separated by a special word token (a period). This allows us to generate\\nphrases that are much longer than the phrases encountered in the SCAN dataset and also\\ncontrol the variance of phrase lengths.\\nThe eSCAN dataset allows us to gradually increase a phrase’s complexity while having control over\\nphrase lengths. At its simplest, eSCAN can also be one-to-one if one restricts to input phrases with\\nsome combination of the words ‘run’, ‘walk’, ‘jump’, and ‘look’.\\nThroughout the main text, we use the subset of SCAN that contains the words ‘run’, ‘walk’, ‘jump’,\\n‘and’, ‘left’, and ‘twice’ with lengths ranging from 10 to 15. Furthermore, we omit combinations that\\n15'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 15}, page_content='require a composition of rules, e.g. the phrase ‘jump left twice’ that requires the network to both\\nunderstand the reversing of ‘jump left’ and the repetition of ‘twice’. Although it would be interesting\\nto study if and how the network learns such compositions, we leave such studies for future work.\\nThis results in 90 distinct subphrases prior to concatenation, with a maximum input and output length\\nof 5 and 4, respectively. Post concatenation, there are over a million distinct phrases for phrases of\\nlength 10 to 15. Once again, one-hot encoding is used for the input and output phrases and train/test\\nsets are generated dynamically.\\nTranslation Dataset Our natural-language translation dataset is the ParaCrawl Corpus, or Web\\nScale Parallel Corpora for European Languages (Bañón et al., 2020); we train models to translate\\nbetween English and French, using the release of the dataset available in TensorFlow Datasets. This\\nrelease of ParaCrawl features 31,374,161 parallel English/French sentences; as we train our models\\nfor 30,000 steps using a batch size of 64, we do not encounter all examples during training.\\nTo aid interpretability, we tokenize the dataset at the word level, by ﬁrst converting all characters\\nto lowercase, separating punctuation from words, and splitting on whitespace. Using 10 million\\nrandomly chosen sentences from the dataset, we build a vocabulary consisting of the 30,000 most\\ncommonly occurring words. We ﬁlter sentences which are longer than 15 tokens.\\nA.4 Recurrent Neural Networks\\nThe three types of RNNs we use in this work are speciﬁed below. Wandbrepresent trainable\\nweight matrices and bias parameters, respectively, and htdenotes the hidden state at timestep t\\n(representing either the encoder or decoder). All other vectors ( ct,gt,rt,it,ft) represent intermediate\\nquantities at time step t;σ(·)represents a pointwise sigmoid nonlinearity; and f(·)is the pointwise\\ntanh nonlinearity.\\nGated Recurrent Unit (GRU) The hidden state update expression for the GRU Cho et al. (2014)\\nis given by\\nht=gt·ht−1+ (1−gt)·ct, (18a)\\nwith\\nct=f(\\nWch(r·ht−1) +Wcxxt+bc)\\n, (18b)\\ngt=σ(\\nWghht−1+Wgxxt+bg)\\n, (18c)\\nrt=σ(\\nWrhht−1+Wrxxt+br)\\n, (18d)\\nUpdate-Gate RNN (UGRNN) The hidden state update expression for the UGRNN Collins et al.\\n(2016) is given by\\nht=gt·ht−1+ (1−gt)·ct, (19a)\\nwith\\nct=f(\\nWchht−1+Wcxxt+bc)\\n, (19b)\\ngt=σ(\\nWghht−1+Wgxxt+bg)\\n, (19c)\\nLong-Short-Term-Memory (LSTM) Unlike the GRU and the UGRNN, the LSTM Hochreiter &\\nSchmidhuber (1997) transfers both a “hidden state” and a cell state from one time step to the next. In\\norder to cast the LSTM update expressions into the same form as the GRU and UGRNN, we deﬁne\\nits hidden state to be\\nht=[\\nct,˜ht]\\n, (20a)\\nwith the update expression given by\\n˜ht=f(ct)·σ(\\nWhhht−1+Whxxt+bh)\\n, (20b)\\nct=ft·ct−1+i·σ(\\nWch˜ht−1+Wcxxt+bc)\\n, (20c)\\nit=σ(\\nWihht−1+Wixxt+bi)\\n, (20d)\\nft=σ(\\nWfhht−1+Wfxxt+bf)\\n. (20e)\\n16'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 16}, page_content='For the LSTM, we only use ˜hE\\ntand˜hD\\nsfor the determination of the context vector and as the decoder\\noutput that is passed to the readout. That is, for AED, ys=W[˜hD\\ns,cs] +bwithcsthe same as (5)\\nwith all hE\\nt→˜hE\\ntand the alignment ast:=˜hD\\ns·˜hE\\nt.\\nA.5 Training\\nFor the one-to-one and eSCAN datasets, we train networks with the ADAM optimizer (Kingma & Ba,\\n2014) and an exponentially-decaying learning rate schedule with an initial learning rate of η= 0.1\\nand a decay rate of 0.9997 every step (with the exception of the VED networks, in which case we\\nused a decay rate of 0.9999 ). Cross-entropy loss with ℓ2regularization was used and gradients were\\nclipped at a maximum value of 10. Both datasets used a batch size of 128and each dynamically\\ngenerated dataset was trained over two epochs. For the AED and VED architectures, a hidden\\ndimension size of n= 128 was used, while for the AO we used n= 256 (for LSTM cells, both the\\nhidden-state˜htand the memory ctaren-dimensional). For these synthetic experiments, we do not\\nadd a bias term to this linear readout layer for the purposes of simplicity and ease of interpretation.\\nAll training for these tasks was performed on GPUs and took at most 20 minutes.\\nAs mentioned above, due to the small vocabulary size of one-to-one and eSCAN, we simply pass\\none-hot encoded inputs in the RNN architectures, i.e. we use no embedding layer. For the AO\\narchitecture, the input dimension was padded up 50and100for the one-to-one and eSCAN tasks,\\nrespectively. Positional encoding vectors were rotated by a random orthonormal matrix so they\\nwere misaligned with the one-hot-encoded input vectors. Finally, we found performance improved\\nwhen positional encoding time-scale τwas chosen to be of order the phrase length: τ= 50 for the\\none-to-one tasks and τ= 100 for the eSCAN tasks.\\nFor the natural translation datasets, each token was mapped to an embedded vector of dimension\\n128 using a learned embedding layer. Both the AED and AO architectures used GRU cells, with\\nhidden dimension size of n= 128 . As in the synthetic datasets, we train using the ADAM optimizer\\nfor 30,000 steps using a batch size of 64; we use an exponential learning rate schedule with initial\\nη= 0.01and a decay rate of 0.99995. Gradients are clipped to a maximum value of 30.\\nB Additional Results\\nIn this section, we discuss several additional results that supplement those discussed in the main text.\\nB.1 LSTM and UGRNN Cells\\nIn the main text, all plots used a GRU RNN for the cells in the encoder and decoder. For the UGRNN\\nand LSTM we ﬁnd qualitatively similar results on the one-to-one task. Summaries of our results for\\nthe two RNN cells are shown in Fig. 8 and Fig. 9, respectively. For all types of cells, the networks\\nachieve 100% test accuracy on the one-to-one task.\\nNotably, for the AED architecture, we observe that the LSTM has a slightly different attention matrix\\nthan that of the GRU and UGRNN. Namely, for the ﬁrst few decoder time steps, the LSTM appears\\nto attend to hE\\nTof a given phrase. This means that the network is transferring information about\\nthe ﬁrst few inputs all the way to the ﬁnal encoder hidden state. As such, it appears that the AED\\narchitecture with LSTM cells relies more on recurrence to solve the one-to-one task relative to its\\nGRU and UGRNN counterparts. Similar to our ﬁndings for the VED architecture in the main text,\\nwe ﬁnd the input-delta components to be signiﬁcantly less clustered around their corresponding\\ninput components when this occurs (Fig. 9b). In contrast, the AO architecture with LSTM cells is\\nqualitatively similar to that with the GRU or UGRNN cells.\\nWe also train the AO architecture with LSTM and UGRNN cells on eSCAN. We again see qualitatively\\nsimilar behavior to what we saw in the main text for AO with GRU cells (Fig. 10). For both types\\nof cells, we see the temporal components align to form an approximately diagonal attention matrix\\n(Fig. 10b,e). Once again, we see the input-delta components to be closely clustered around their\\ncorresponding input components, and said input components are close to their respective readouts,\\nimplementing the translation dictionary (Fig. 10c,f).\\n17'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 17}, page_content='Figure 8: Summary of dynamics for all three architectures on the one-to-one task with UGRNN cells.\\nAll three architectures trained on an N= 3one-to-one task of variable length ranging from 15to20.(a)For\\nAED, the path formed by the temporal components of the encoder (orange) and decoder (purple), µE\\ntandµD\\ns.\\nWe denote the ﬁrst and last temporal component by a square and star, respectively, and the color of said path is\\nlighter for earlier times. The inset shows the softmaxed alignment scores for µD\\ns·µE\\nt, which we ﬁnd to be a\\ngood approximation to the full alignment for the one-to-one task. (b)The input-delta components of the encoder\\n(light) and decoder (dark) colored by word (see labels). The encoder input components, χE\\nxare represented by a\\ndark colored ‘X’. The solid lines are the readout vectors (see labels on (d)). (c, d) The same plots for the AO\\nnetwork. (e, f) The same plots for the VED network (with no attention inset).\\nFigure 9: Summary of dynamics for all three architectures on the one-to-one translation task with LSTM\\ncells. All three architectures trained on an N= 3 one-to-one translation task with inputs of variable length\\nranging from 15to20.(a)For AED, the path formed by the temporal components of the encoder (orange) and\\ndecoder (purple), µE\\ntandµD\\ns. We denote the ﬁrst and last temporal component by a square and star, respectively,\\nand the color of said path is lighter for earlier times. The inset shows the softmaxed alignment scores for\\nµD\\ns·µE\\nt, which we ﬁnd to be a good approximation to the full alignment for the one-to-one task. (b)The\\ninput-delta components of the encoder (light) and decoder (dark) colored by word (see labels). The encoder\\ninput components, χE\\nxare represented by a dark colored ‘X’. The solid lines are the readout vectors (see labels\\non (d)). (c, d) The same plots for the AO network. (e, f) The same plots for the VED network (with no attention\\ninset).\\n18'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 18}, page_content='B.2 Autonomous Dynamics and Temporal Components\\nWhen the temporal components are the leading-order behavior in alignment scores (e.g. in the\\none-to-one and eSCAN tasks), we ﬁnd them to be well approximated the hidden states resulting from\\nthe network having zero input, i.e. xE\\nt=0for allt. In the case of AED and VED, this means the\\nencoder RNNs are driven solely by their recurrent behavior. For AO, the encoder RNNs are driven by\\nonly the positional encoding vectors. Since in all three cases this results network outputting hidden\\nstates independent of the details of the input, we call this the autonomous dynamics of the network.\\nDenote the encoder and decoder hidden states resulting from no input by hE,0\\ntandhD,0\\ns, respectively.\\nFor AED and VED, they are\\nhE,0\\nt=FE(hE,0\\nt−1,0), hD,0\\ns=FD(hD,0\\ns−1,0). (21)\\nFor the AO network, we still add the positional encoding vectors to the inputs,\\nhE,0\\nt=FE(0,pE\\nt), hD,0\\ns=FD(0,pD\\ns). (22)\\nPlotting the resulting hidden states along with the temporal components, we ﬁnd for all three\\narchitectures the two quantities are quite close at all time steps of the encoder and decoder (Fig. 11).\\nWe quantify the degree to which the null hidden states approximate the temporal components via\\n∥hE\\nt−hE,0\\nt∥2/∥hE\\nt∥2and∥hD\\ns−hD,0\\ns∥2/∥hD\\ns∥2where∥·∥2denotes theℓ2-norm. For the AO network,\\nwe ﬁnd the average of this quantity across the entire encoder and decoder phrases to be about 0.07\\nand0.08, respectively, while for AED we ﬁnd it to be 0.07for the encoder and 0.19for the decoder.\\nWe also ﬁnd the null hidden states to be close to the temporal components of eSCAN (Fig. 12). Again,\\naveraging our difference measure across the entire encoder and decoder phrases, for AO we ﬁnd 0.11\\nand0.15and for AED 0.21and0.17, respectively.\\nThis result gives insight into the network dynamics that drive the behavior of the temporal components.\\nIn AED and VED, each RNN cell is driven by two factors: the recurrent hidden state and the input.\\nThe AO architecture is similar, but the recurrence is replaced by temporal information through the\\npositional encoding. The absence of input eliminates the input-driven behavior in the RNN cells.\\nSince the network’s hidden states still trace out paths very close to the temporal components, this is\\nevidence that it is the recurrent dynamics (in the case of AED and VED) or the positional encoding\\nvectors that drive the network’s temporal component behavior.\\nOne can use this information to postulate other behaviors in the network. For instance, given the lack\\nof correlation between inputs in the one-to-one translation task, in the AO network one may wonder\\nhow much of the decoder’s dynamics are driven by recurrent versus input behavior. We already know\\nthe decoder’s primary job in this network is to align its temporal components with that of the encoder,\\nand the results above suggest said behavior is driven primarily by the positional encoding vectors and\\nnot the inputs. To test this, we compare the accuracies of a trained network with and without inputs\\ninto the decoder RNN. We still achieve 100% word accuracy when xD\\ns=0for all decoder time steps.\\nB.3 Learned Attention\\nIn addition to the dot-product attention considered throughout the main text, we have implemented\\nand analyzed new architectures that are identical to the AED and AO architectures that use a learned-\\nattention mechanism. These networks use a scaled-dot product attention in the form of queries, keys,\\nand value matrices similar to the original Transformer Vaswani et al. (2017). More speciﬁcally, the\\ncontext vector csand alignment astare now determined by the expressions\\ncs:=T∑\\nt=1αstvt, ast:=qs·vt. (23)\\nIn these expressions, the vectors vt,qs, andktare product of the learned weight matrices V∈Rn×n,\\nQ∈Rn′×n, andK∈Rn′×nand the hidden states,\\nvt:=VhE\\nt, qs:=QhD\\ns, kt:=KhE\\nt, (24)\\nwithvt∈Rn(i.e. the same dimension as hidden state space) and qs,kt∈Rn′, withn′the dimension\\nof the query/key space.\\n19'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 19}, page_content='Figure 10: Summary of dynamics for AO architectures with LSTM and UGRNN cells trained on eSCAN.\\n(a)Example attention matrix for the AED architecture. (b)AED network’s temporal components, with the inset\\nshowing the attention matrix from said temporal components. Once again, encoder and decoder components are\\norange and purple, respectively. (c)AED network’s input-delta components, input components, and readouts, all\\ncolored by their corresponding input/output words (see labels). (d, e, f) The same plots for UGRNN cells.\\nFigure 11: Autonomous dynamics versus temporal components for architectures trained on one-to-one\\ntranslation. All three architectures are trained on an N= 3one-to-one translation task with inputs of variable\\nlength ranging from 15to20.(a)For AED, the path formed by the temporal components of the encoder (orange),\\nµE\\nt. Also plotted in green are the null hidden states, hE,0\\nt. We denote the ﬁrst and last hidden state of each of\\nthese by a square and star, respectively. The inset shows the quantitative difference between the two states,\\n∥hE\\nt−hE,0\\nt∥2/∥hE\\nt∥2, as a function of encoder time step, t.(b)The same plot but for the decoder temporal\\ncomponents (purple) and the decoder null hidden states (red). (c, d) The same plots for the AO network. (e, f)\\nThe same plots for the VED network.\\n20'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 20}, page_content='Figure 12: Autonomous dynamics versus temporal components for architectures trained on eSCAN. (a)\\nFor AED, the path formed by the temporal components of the encoder (orange), µE\\nt. Also plotted in green\\nare the null hidden states, hE,0\\nt. We denote the ﬁrst and last hidden state of each of these by a square and star,\\nrespectively. The inset shows the quantitative difference between the two states, ∥hE\\nt−hE,0\\nt∥2/∥hE\\nt∥2, as a\\nfunction of encoder time step, t.(b)The same plot but for the decoder temporal components (purple) and the\\ndecoder null hidden states (red). (c, d) The same plots for the AO network.\\nAfter training these networks on our one-to-one and eSCAN tasks, we ﬁnd very similar results to that\\nof dot-product attention. In particular, temporal components of the encoder and decoder continue to\\nalign with one another after being projected through the respective key/query matrix (for AO, see\\nFig. 13a). Input-delta components cluster along the respective readouts, after being projected through\\nthe value matrix (Fig. 13b). Decomposing the alignment scores for these networks, we continue to\\nﬁnd them to be dominated by the temporal component term (i.e. µD\\nsQTKµE\\nt).\\nThis gives us additional conﬁdence that the analysis techniques can be applied to modern architectures\\nthat use attention. For instance, in a multi-headed attention setting, such a decomposition could be\\nused on each head separately to characterize the dynamics behind each of the heads.\\nB.4 Attention Only with Non-Gated Feedforward\\nThe AO architecture’s feedfoward networks are created by zeroing the recurrent part of various RNNs.\\nAlthough this does result in a feedforward network, the presence of the gating mechanisms in the\\nRNN cells we use in this work make this non-standard feedforward network. To verify our qualitative\\nresults hold beyond a gated feedfoward network, we investigated if our results differed when using a\\nstandard fully connected layer followed by a tanh readout.\\nWe ﬁnd the non-gated AO network trains well on both the one-to-one and eSCAN tasks, achieving\\n100% and 98.8% word accuracy, respectively. Again performing the temporal and input component\\ndecomposition on the network’s hidden states, we ﬁnd the qualitative dynamics of this network are\\nthe same as its RNN counterparts (Fig. 14). For example, in the one-to-one translation task we ﬁnd\\nthe temporal components of the encoder and decoder again mirror one another in order to from a\\ndiagonal attention matrix (Fig. 14a). The input components of the encoder align with the readouts to\\nimplement the translation dictionary (Fig. 14b).\\n21'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 21}, page_content='Figure 13: Dynamics for AO with learned attention. (a) The path formed by the temporal components of\\nthe encoder (orange) and decoder (purple) multiplied by the key and query matrices, respectively, i.e. KµE\\ntand\\nQµD\\ns. We denote the ﬁrst and last temporal component by a square and star, respectively, and the color of said\\npath is lighter for earlier times. The inset shows the softmaxed alignment scores for qs·kt, which we ﬁnd to be\\na good approximation to the full alignment for the one-to-one translation task. (b)The input-delta components\\nof the encoder (light) and decoder (dark) colored by word (see labels), after being multiplied by value matrix,\\nV. The encoder input components, VχE\\nxare represented by a dark colored ‘X’. The solid lines are the readout\\nvectors.\\nB.5 Encoder-Decoder with Attention Readouts\\nThe AED network’s linear readout takes into account both the decoder hidden state output and the\\ncontext vector, i.e. ys=W[hD\\ns,cs]. As such, each output’s readout can be viewed as twovectors\\nin hidden state space: one which acts on the context vector and the other which acts on the decoder\\nhidden state output. Here we elaborate on our comment in the main text regarding the effects of the\\ndecoder readouts being negligible.\\nRecall that after training the AED network, we found the context vector readouts align with the\\nencoder input components, yielding the translation dictionary for a given task. We ﬁnd the the decoder\\nreadouts to be close to orthogonal to the context vector readouts (Fig. 15). Furthermore, we ﬁnd the\\nreadouts for all words other than the ‘eos’ character to be closely aligned. Omitting the ‘eos‘ effect,\\nthis results in the decoder readouts contributing roughly equal values to all logit values. Since the\\nlogit values are passed through a softmax function, it is their differences that ultimately matter when\\nit comes to classifying a given output as a word. In contrast, we found the context vector readouts to\\nalign with the vertices of an (N−1)-simplex and the logit value contributions to vary signiﬁcantly\\nmore with the hidden state. Indeed, comparing the difference between the largest and second largest\\nlogit contribution of each set of readouts, we ﬁnd the difference due to the context vector readouts to\\nbe several times larger than that of the decoder readouts. For AED trained on eSCAN, we again ﬁnd\\nthe differences in the context vector logit contributions to be several times larger than that due to the\\ndecoder hidden states.\\nB.6 Vanilla Encoder-Decoder Dynamics\\nIn the main text, we brieﬂy discussed the dynamics of the VED architecture, and here we provide\\nsome additional details. After training the VED architecture on the one-to-one translation task, we\\nfound that the encoder and decoder hidden states belonging to the same time step formed clusters,\\nand said clusters are closest to those corresponding to adjacent time steps. Additionally, since the\\nVED arhcitecture has no attention, the encoder and decoder hidden states have to carry all relevant\\ninformation from preceding steps. To facilitate this, the dynamics of the encoder’s hidden state space\\norganizes itself into a tree structure to encode the input phrases (Fig. 16). Starting from the encoder’s\\ninitial hidden state, the hidden states of a given input phrase traverse the branches of said tree as the\\nphrase is read in, ultimately arriving at one of the tree’s leaves at time T. The distinct leaves represent\\nthe different ﬁnal encoder hidden states, hE\\nT, that must encode the input phrase’s words and their\\nordering.\\nSince the decoder receives no additional input, the encoder must place hE\\nTin a location of hidden\\nstate space for the decoder’s dynamics to produce the entire output phrase. Although the network\\ndoes indeed learn to do this, we do not observe the reversal of the tree structure learned by the\\n22'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 22}, page_content='Figure 14: AO with non-gated feed foward trained on one-to-one and eSCAN. (a) For the AO trained on\\nanN= 3one-to-one translation task, the path formed by the temporal components of the encoder (orange) and\\ndecoder (purple), µE\\ntandµD\\ns. We denote the ﬁrst and last temporal component by a square and star, respectively,\\nand the color of said path is lighter for earlier times. The inset shows the softmaxed alignment scores for µD\\ns·µE\\nt,\\nwhich we ﬁnd to be a good approximation to the full alignment for the one-to-one translation task. (b)The\\ninput-delta components of the encoder (light) and decoder (dark) colored by word (see labels). The encoder\\ninput components, χE\\nxare represented by a dark colored ‘X’. The solid lines are the readout vectors (see labels).\\n(c, d) The same plots for AO trained on eSCAN.\\nencoder. That is, any two phrases that have the same output sequence for any time s≥s′could\\noccupy the same decoder hidden states ˜hsfors≥s′. This would result in the temporal mirror of the\\ntree structure seen in the encoder. However, such a structure is not observed and the decoder instead\\nseems to arrive at a solution where all output paths are kept distinct.\\n23'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 23}, page_content='Figure 15: Decoder hidden state behavior in AED trained on one-to-one translation. The input-delta\\ncomponents of the encoder (light) and decoder (dark) colored by word. The encoder input components, χE\\nx\\nare represented by a dark colored ‘X’. The solid lines are the readout vectors, with those corresponding to the\\ncontext vector colored dark (and labeled by ‘RO’) and those corresponding to the decoder hidden state readout\\ncolored light (and labeled by ‘Dec RO’).\\nFigure 16: Tree-like structure of encoder hidden states in VED. The encoder hidden states of the VED\\narchitecture trained on the one-to-one translation task for the ﬁrst ﬁve time steps. Note the hidden states organize\\nthemselves in ﬁve distinct clusters along PC 0. Also shown are the paths in hidden state space various input\\nphrases take as a given input phrase is read. Said paths are colored from red to black by similarity starting from\\nlatest time, i.e.{B,A,C,A,C}and{B,A,C,A,B}are similarly colored but {C,A,C,A,C}is not.\\n24')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting steps 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'encoder-decoder.pdf', 'page': 0}, page_content='Understanding How Encoder-Decoder\\nArchitectures Attend\\nKyle Aitken\\nDepartment of Physics\\nUniversity of Washington\\nSeattle, Washington, USA\\nkaitken17@gmail.comVinay V Ramasesh\\nGoogle Research, Blueshift Team\\nMountain View, California, USA\\nYuan Cao\\nGoogle Research, Brain Team\\nMountain View, California, USANiru Maheswaranathan\\nGoogle Research, Brain Team\\nMountain View, California, USA\\nAbstract\\nEncoder-decoder networks with attention have proven to be a powerful way to solve\\nmany sequence-to-sequence tasks. In these networks, attention aligns encoder and\\ndecoder states and is often used for visualizing network behavior. However, the\\nmechanisms used by networks to generate appropriate attention matrices are still\\nmysterious. Moreover, how these mechanisms vary depending on the particular\\narchitecture used for the encoder and decoder (recurrent, feed-forward, etc.) are also\\nnot well understood. In this work, we investigate how encoder-decoder networks'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 0}, page_content='architecture used for the encoder and decoder (recurrent, feed-forward, etc.) are also\\nnot well understood. In this work, we investigate how encoder-decoder networks\\nsolve different sequence-to-sequence tasks. We introduce a way of decomposing\\nhidden states over a sequence into temporal (independent of input) and input-\\ndriven (independent of sequence position) components. This reveals how attention\\nmatrices are formed: depending on the task requirements, networks rely more\\nheavily on either the temporal or input-driven components. These ﬁndings hold\\nacross both recurrent and feed-forward architectures despite their differences in\\nforming the temporal components. Overall, our results provide new insight into the\\ninner workings of attention-based encoder-decoder networks.\\n1 Introduction\\nModern machine learning encoder-decoder architectures can achieve strong performance on sequence-\\nto-sequence tasks such as machine translation (Bahdanau et al., 2014; Luong et al., 2015; Wu et al.,'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 0}, page_content='Modern machine learning encoder-decoder architectures can achieve strong performance on sequence-\\nto-sequence tasks such as machine translation (Bahdanau et al., 2014; Luong et al., 2015; Wu et al.,\\n2016; Vaswani et al., 2017), language modeling (Raffel et al., 2020), speech-to-text (Chan et al.,\\n2015; Prabhavalkar et al., 2017; Chiu et al., 2018), etc. Many of these architectures make use of\\nattention (Bahdanau et al., 2014), a mechanism that allows the network to focus on a speciﬁc part of\\nthe input most relevant to the current prediction step. Attention has proven to be a critical mechanism;\\nindeed many modern architectures, such as the Transformer, are fully attention-based (Vaswani et al.,\\n2017). However, despite the success of these architectures, an understanding of how said networks\\nsolve such tasks using attention remains largely unknown.\\nAttention mechanisms are attractive because they are interpretable, and often illuminate key com-'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 0}, page_content='solve such tasks using attention remains largely unknown.\\nAttention mechanisms are attractive because they are interpretable, and often illuminate key com-\\nputations required for a task. For example, consider neural machine translation—trained networks\\nexhibit attention matrices that align words in the encoder sequence with the appropriate correspond-\\ning position in the decoder sentence (Ghader & Monz, 2017; Ding et al., 2019). In this case, the\\nattention matrix already contains information about which words in the source sequence are relevant\\nfor translating a particular word in the target sequence; that is, forming the attention matrix itself\\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).arXiv:2110.15253v1  [cs.LG]  28 Oct 2021'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 1}, page_content='constitutes a signiﬁcant part of solving the overall task. How is it that networks are able to achieve\\nthis? What are the mechanisms underlying how networks form attention, and how do they vary across\\ntasks and architectures?\\nIn this work, we study these questions by analyzing three different encoder-decoder architectures on\\nsequence-to-sequence tasks. We develop a method for decomposing the hidden states of the network\\ninto a sum of components that let us isolate input driven behavior from temporal (or sequence) driven\\nbehavior. We use this to ﬁrst understand how networks solve tasks where all samples use the same\\nattention matrix, a diagonal one. We then build on that to show how additional mechanisms can\\ngenerate sample-dependent attention matrices that are still close to the average matrix.\\nOur Contributions\\n•We propose a decomposition of hidden state dynamics into separate pieces, one of which explains'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 1}, page_content='Our Contributions\\n•We propose a decomposition of hidden state dynamics into separate pieces, one of which explains\\nthe temporal behavior of the network, another of which describes the input behavior. We show\\nsuch a decomposition aids in understanding the behavior of networks with attention.\\n•In the tasks studied, we show the temporal (input) components play a larger role in determining\\nthe attention matrix as the average attention matrix becomes a better (worse) approximation for a\\nrandom sample’s attention matrix.\\n•We discuss the dynamics of architectures with attention and/or recurrence and show how the\\ninput/temporal component behavior differs across said architectures.\\n•We investigate the detailed temporal and input component dynamics in a synthetic setting to\\nunderstand the mechanism behind common sequence-to-sequence structures and how they might\\ndiffer in the presence of recurrence.\\nRelated Work As mentioned in the introduction, a common technique to gain some understanding'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 1}, page_content='differ in the presence of recurrence.\\nRelated Work As mentioned in the introduction, a common technique to gain some understanding\\nis to visualize learned attention matrices, though the degree to which such visualization can explain\\nmodel predictions is disputed Wiegreffe & Pinter (2019); Jain & Wallace (2019); Serrano & Smith\\n(2019). Input saliency Bastings & Filippova (2020) and attribution-propagation Chefer et al. (2020)\\nmethods have also been studied as potential tools for model interpretability.\\nComplementary to these works, our approach builds on a recent line of work analyzing the computa-\\ntional mechanisms learned by RNNs from a dynamical systems perspective. These analyses have iden-\\ntiﬁed simple and interpretable hidden state dynamics underlying RNN operation on text-classiﬁcation\\ntasks such as binary sentiment analysis (Maheswaranathan et al., 2019; Maheswaranathan & Sussillo,'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 1}, page_content='tiﬁed simple and interpretable hidden state dynamics underlying RNN operation on text-classiﬁcation\\ntasks such as binary sentiment analysis (Maheswaranathan et al., 2019; Maheswaranathan & Sussillo,\\n2020) and document classiﬁcation (Aitken et al., 2020). Our work extends these ideas into the domain\\nof sequence-to-sequence tasks.\\nNotation LetTandSbe the input and output sequence length of a given sample, respectively.\\nWe denote the encoder and decoder hidden states by hE\\nt∈Rnwitht= 1,...,T . Similarly, we\\ndenote decoder hidden states by hD\\ns∈Rn, withs= 1,...,S . The encoder and decoder hidden state\\ndimensions are always taken to be equal in this work. Inputs to the encoder and decoder are denoted\\nbyxE\\nt∈RdandxD\\ns∈R˜d. When necessary, we subscript different samples from a test/train set using\\nα,β,γ , e.g.xE\\nt,αforα= 1,...,M .\\nOutline We begin by introducing the three architectures we investigate in this work with varying'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 1}, page_content='α,β,γ , e.g.xE\\nt,αforα= 1,...,M .\\nOutline We begin by introducing the three architectures we investigate in this work with varying\\ncombinations of recursion and attention. Next we introduce our temporal and input component\\ndecomposition and follow this up with a demonstration of how such a decomposition allows us to\\nunderstand the dynamics of attention in a simple one-to-one translation task. Afterwards, we apply\\nthis decomposition to two additional tasks with increasing levels of complexity and discuss how our\\ndecomposition gives insight into the behavior of attention in these tasks.\\n2 Setup\\nA schematic of the three architectures we study is shown in Fig. 1 (see Appendix A.1 for precise\\nexpressions).\\nVanilla Encoder Decoder (VED) is a recurrent encoder-decoder architecture with no attention\\n(Sutskever et al., 2014). The encoder and decoder update expression are hE\\nt=FE(hE\\nt−1,xE\\nt)and\\nhD\\ns=FD(hD\\ns−1,xD\\ns), respectively. Here, FDandFEare functions that implement the hidden\\n2'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 2}, page_content='Figure 1: Schematic of the three primary architectures analyzed in this work. The orange, purple, and\\ngreen boxes represent the encoder RNNs, decoder RNNs, and linear readout layers, respectively. Recurrent\\nconnections are shown in blue, attention-based connections and computational blocks are shown in gold. The\\ngrey circles add positional encoding to the inputs.\\nstate updates, which in this work are each one of three modern RNN cells: LSTMs (Hochreiter &\\nSchmidhuber, 1997), GRUs (Cho et al., 2014), or UGRNNs (Collins et al., 2016).\\nEncoder-Decoder with Attention (AED) is identical to the VED architecture above with a simple\\nattention mechanism added (Bahdanau et al., 2014; Luong et al., 2015). For time step sof the decoder,\\nwe compute a context vector cs, a weighted sum of encoder hidden states, cs:=∑T\\nt=1αsthE\\nt, with\\nαt:=softmax (a1t,...,aSt)thetthcolumn of the attention matrix andast:=hD\\ns·hE\\ntthealignment'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 2}, page_content='we compute a context vector cs, a weighted sum of encoder hidden states, cs:=∑T\\nt=1αsthE\\nt, with\\nαt:=softmax (a1t,...,aSt)thetthcolumn of the attention matrix andast:=hD\\ns·hE\\ntthealignment\\nbetween a given decoder and encoder hidden state. While more complicated attention mechanisms\\nexist, in the main text we analyze the simplest form of attention for convenience of analysis.1\\nAttention Only (AO) is identical to the AED network above, but simply eliminates the recurrent\\ninformation passed from one RNN cell to the next and instead adds ﬁxed positional encoding vectors\\nto the encoder and decoder inputs (Vaswani et al., 2017). Due to the lack of recurrence, the RNN\\nfunctionsFEandFDsimply act as feedforward networks in this setting.2AO can be treated as a\\nsimpliﬁed version of a Transformer without self-attention, hence our analysis may also provide a hint\\ninto their inner workings (Vaswani et al., 2017).\\n2.1 Temporal and Input Components'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 2}, page_content='simpliﬁed version of a Transformer without self-attention, hence our analysis may also provide a hint\\ninto their inner workings (Vaswani et al., 2017).\\n2.1 Temporal and Input Components\\nIn architectures with attention, we will show that it is helpful to write the hidden states using what we\\nwill refer to as their temporal andinput components. This will be useful because each hidden state\\nhas an associated time step and input word at that same time step (e.g. sandxD\\nsforhD\\ns), therefore\\nsuch a decomposition will often allow us to disentangle temporal and input behavior from any other\\nnetwork dynamics.\\nWe deﬁne the temporal components of the encoder and decoder to be the average hidden state at a\\ngiven time step, which we denote by µE\\ntandµD\\ns, respectively. Similarly, we deﬁne an encoder input\\ncomponent to be the average of all hE\\nt−µE\\ntfor hidden states that immediately follow a given input\\nword. We analogously deﬁne the decoder input components. In practice, we estimate such averages'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 2}, page_content='component to be the average of all hE\\nt−µE\\ntfor hidden states that immediately follow a given input\\nword. We analogously deﬁne the decoder input components. In practice, we estimate such averages\\nusing a test set of size M, so that the temporal and input components of the encoder are respectively\\ngiven by\\nµE\\nt≈∑M\\nα=11≤EoS,αhE\\nt,α∑M\\nβ=11≤EoS,β, χE(xt,α)≈∑M\\nβ=1∑T\\nt′=11xt,α,xt′,β(\\nhE\\nt′,β−µE\\nt′)\\n∑M\\nγ=1∑T\\nt′′=11xt,α,xt′′,γ,(1)\\nwhere hE\\nt,αthe encoder hidden state of the αth sample, 1≤EoS,αis a mask that is zero if the αth\\nsample is beyond the end of sentence, 1xt,α,xt′,βis a mask that is zero if xt,α̸=xt′,β, and we\\n1In Appendix B.3, we implement a learned-attention mechanism using a scaled-dot product attention in the\\nform of queries, keys, and value matrices (Vaswani et al., 2017). For the AED and AO architectures, we ﬁnd\\nqualitatively similar results to the simple dot-product attention presented in the main text.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 2}, page_content='qualitatively similar results to the simple dot-product attention presented in the main text.\\n2We train non-gated feedforward networks and ﬁnd their dynamics to be qualitatively the same, see Appendix\\nB.\\n3'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 3}, page_content='Figure 2: Summary of attention dynamics on synthetic tasks. (a-f) All three architectures trained on an\\nN= 3one-to-one translation task of variable length ranging from 15to20. Plots in the top row are projected\\nonto the principal components (PCs) of the encoder and decoder temporal components, while those in the\\nbottom row are projected onto the PCs of the input components. (a)For AED, the path formed by the temporal\\ncomponents of the encoder (orange) and decoder (purple), µE\\ntandµD\\ns. We denote the ﬁrst and last temporal\\ncomponent by a square and star, respectively, and the color of said path is lighter for earlier times. The inset\\nshows the softmaxed alignment scores for µD\\ns·µE\\nt, which we ﬁnd to be a good approximation to the full\\nalignment for the one-to-one translation task. (b)The input-delta components of the encoder (light) and decoder\\n(dark) colored by word (see labels). The encoder input components, χE\\nxare represented by a dark colored ‘X’.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 3}, page_content='(dark) colored by word (see labels). The encoder input components, χE\\nxare represented by a dark colored ‘X’.\\nThe solid lines are the readout vectors (see labels on (d)). Start/end of sentence characters are in purple. (c, d)\\nThe same plots for the AO network. (e, f) The same plots for the VED network (with no attention inset). (g)\\nTemporal components for the same task with a temporally reversed output sequence. (h)Attention matrices\\nfor a test example from a network trained to alphabetically sort a list of letters. Clockwise from top left, the\\nsoftmaxed attention from the full hidden states ( hD\\ns·hE\\nt), temporal components only ( µD\\ns·µE\\nt), decoder input\\ncomponents and encoder delta components ( χD\\ny·∆hE\\nt), and decoder delta components and encoder input\\ncomponents ( ∆hD\\ns·χE\\nx).\\nhave temporarily suppressed superscripts on the inputs for brevity.3By deﬁnition, the temporal\\ncomponents only vary with time and the input components only vary with input/output word . As such,'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 3}, page_content='x).\\nhave temporarily suppressed superscripts on the inputs for brevity.3By deﬁnition, the temporal\\ncomponents only vary with time and the input components only vary with input/output word . As such,\\nit will be useful to denote the encoder and decoder input components by χE\\nxandχD\\ny, withxandy\\nrespectively running over all input and output words (e.g. χE\\nyesandχD\\noui). We can then write any\\nhidden state as\\nhE\\nt=µE\\nt+χE\\nx+ ∆hE\\nt,hD\\ns=µD\\ns+χD\\ny+ ∆hD\\ns, (2)\\nwith ∆hE\\nt:=hE\\nt−µE\\nt−χE\\ntand∆hD\\ns:=hD\\ns−µD\\ns−χD\\nythedelta components of encoder and\\ndecoder hidden states, respectively. Intuitively, we are simply decomposing each hidden state vector\\nas a sum of a component that only varies with time/position in the sequence (independent of input),\\na component that only varies with input (independent of position), and whatever else is left over.\\nFinally, we will often refer to hidden states without their temporal component, i.e. χE\\nx+ ∆hE\\ntand\\nχD\\ny+ ∆hD'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 3}, page_content='Finally, we will often refer to hidden states without their temporal component, i.e. χE\\nx+ ∆hE\\ntand\\nχD\\ny+ ∆hD\\ns, so for brevity we refer to these combinations as the input-delta components .\\nUsing the temporal and input components in (2), we can decompose the attention alignment between\\ntwo hidden states as\\nast=(\\nµD\\ns+χD\\ny+ ∆hD\\ns)\\n·(\\nµE\\nt+χE\\nx+ ∆hE\\nt)\\n. (3)\\nWe will show below that in certain cases several of the nine terms of this expression approximately\\nvanish, leading to simple and interpretable attention mechanisms.\\n3 One-to-One Results\\nTo ﬁrst establish a basis of how each of the three architectures learn to solve tasks and the role of\\ntheir input and temporal components, we start by studying their dynamics for a synthetic one-to-one\\n3See Appendix A.2 for more details on this deﬁnition and the analogous decoder deﬁnitions.\\n4'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 4}, page_content='translation task. The task is to convert a sequence of input words into a corresponding sequence of\\noutput words, where there is a one-to-one translation dictionary, e.g. converting a sequence of letters\\nto their corresponding position in the alphabet, {B,A,C,A,D}→{ 2,1,3,1,4}. We generate the\\ninput phrases to have variable length, but outputs always have equal length to their input (i.e. T=S).\\nWhile a solution to this task is trivial, it is not obvious how each neural network architecture will\\nsolve the task. Although this is a severely simpliﬁed approximation to realistic sequence-to-sequence\\ntasks, we will show below that many of the dynamics the AED and AO networks learn on this task\\nare qualitatively present in more complex tasks.\\nEncoder-Decoder with Attention. After training the AED architecture, we apply the decomposi-\\ntion of (2)to the hidden states. Plotting the temporal components of both the encoder and decoder,'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 4}, page_content='Encoder-Decoder with Attention. After training the AED architecture, we apply the decomposi-\\ntion of (2)to the hidden states. Plotting the temporal components of both the encoder and decoder,\\nthey each form an approximate circle that is traversed as their respective inputs are read in (Fig. 2a).4\\nAdditionally, we ﬁnd the encoder and decoder temporal components are closest to alignment when\\ns=t. We also plot the input components of the encoder and decoder together with the encoder\\ninput-delta components, i.e. χE\\nx+ ∆hE\\nt, and the network’s readout vectors (Fig. 2b).5We see for\\nthe encoder hidden states, the input-delta components are clustered close to their respective input\\ncomponents, meaning for this task the delta components are negligible. Also note the decoder\\ninput-delta components are signiﬁcantly smaller in magnitude than the decoder temporal components.\\nTogether, this means we can approximate the encoder and decoder hidden states as hE\\nt≈µE\\nt+χE\\nx\\nandhD\\ns≈µD'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 4}, page_content='Together, this means we can approximate the encoder and decoder hidden states as hE\\nt≈µE\\nt+χE\\nx\\nandhD\\ns≈µD\\ns, respectively. Finally, note the readout vector for a given output word aligns with the\\ninput components of its translated input word, e.g. the readout for ‘ 1’ aligns with the input component\\nfor ‘A’ (Fig. 2b).6\\nFor the one-to-one translation task, the network learns an approximately diagonal attention matrix,\\nmeaning the decoder at time sprimarily attends to the encoder’s hidden state at t=s. Additionally, we\\nﬁnd the temporal and input-delta components to be close to orthogonal for all time steps, which allows\\nthe network’s attention mechanism to isolate temporal dependence rather than input dependence.\\nSince we can approximate the hidden states as hE\\nt≈µE\\nt+χE\\nxandhD\\ns≈µD\\ns, and the temporal\\nand input components are orthogonal, the alignment in (3)can be written simply as ast≈µD\\ns·µE\\nt.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 4}, page_content='Since we can approximate the hidden states as hE\\nt≈µE\\nt+χE\\nxandhD\\ns≈µD\\ns, and the temporal\\nand input components are orthogonal, the alignment in (3)can be written simply as ast≈µD\\ns·µE\\nt.\\nThis means that the fullattention is completely described by the temporal components and thus\\ninput-independent (this will not necessarily be true for other tasks, as we will see later).\\nWith the above results, we can understand how AED solves the one-to-one translation task. After\\nreading a given input, the encoder hidden state is primarily composed of an input and temporal\\ncomponent that are approximately orthogonal to one another, with the input component aligned with\\nthe readout of the translated input word (Fig. 2b). The decoder hidden states are approximately made\\nup of only a temporal component, whose sole job is to align with the corresponding encoder temporal\\ncomponent. Temporal components of the decoder and encoder are closest to alignment for t=s,'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 4}, page_content='up of only a temporal component, whose sole job is to align with the corresponding encoder temporal\\ncomponent. Temporal components of the decoder and encoder are closest to alignment for t=s,\\nso the network primarily attends to the encoder state hE\\nt=s. The alignment between encoder input\\ncomponents and readouts yields maximum logit values for the correct translation.\\nAttention Only. Now we turn to AO architecture, which is identical to AED except with the recurrent\\nconnections cut, and positional encoding added to the inputs. We ﬁnd that AO has qualitatively\\nsimilar temporal components that give rise to diagonal attention (Fig. 2c) and the input components\\nalign with the readouts (Fig. 2d). Thus AO solves the task in a similar manner as AED. The only\\ndifference is that the temporal components, driven by RNN dynamics in AED, are now driven purely\\nby the positional encoding in AO.\\nVanilla Encoder-Decoder. After training the VED architecture, we ﬁnd the encoder and decoder'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 4}, page_content='by the positional encoding in AO.\\nVanilla Encoder-Decoder. After training the VED architecture, we ﬁnd the encoder and decoder\\nhidden states belonging to the same time step form clusters, and said clusters are closest to those\\ncorresponding to adjacent time steps. This yields temporal components that are close to one another\\n4Here and in plots that follow, we plot the various components using principal component analysis (PCA)\\nprojections simply as a convenient visualization tool. Other than observation that in some cases the temporal/input\\ncomponents live in a low-dimensional subspace, none of our quantitative analysis is dependent upon the PCA\\nprojections. For all one-to-one plots, a large percentage ( >90%) of the variance is explained by the ﬁrst 2 or 3\\nPC dimensions.\\n5ForNpossible input words, the encoder input components align with the vetrices of an (N−1)-simplex,\\nwhich is similar to the classiﬁcation behavior observed in Aitken et al. (2020).'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 4}, page_content='PC dimensions.\\n5ForNpossible input words, the encoder input components align with the vetrices of an (N−1)-simplex,\\nwhich is similar to the classiﬁcation behavior observed in Aitken et al. (2020).\\n6Since in AED we pass both the decoder hidden state and the context vector to the readout, each readout\\nvector is twice the hidden state dimension. We plot only the readout weights corresponding to the context vector,\\nsince generally those corresponding to the decoder hidden state are negligible, see Appendix B for more details.\\n5'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 5}, page_content='Figure 3: Summary of dynamics for AED and AO architectures trained on eSCAN. (a) Example attention\\nmatrix for the AED architecture. (b)AED network’s temporal components, with the inset showing the attention\\nmatrix from said temporal components. Once again, encoder and decoder components are orange and purple,\\nrespectively and we are projecting onto the temporal component PCs. (c)AED network’s input-delta components,\\ninput components, and readouts, all colored by their corresponding input/output words (see labels). All quantities\\nprojected onto input component PCs. (d, e, f) The same plots for AO.\\nfor adjacent times, with µE\\nTnext to µD\\n1(Figs. 2e). Since there is no attention in this architecture,\\nthere is no incentive for the network to align temporal components of the encoder and decoder as we\\nsaw in AED and AO.\\nAs recurrence is the only method of transferring information across time steps, encoder and de-'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 5}, page_content='saw in AED and AO.\\nAs recurrence is the only method of transferring information across time steps, encoder and de-\\ncoder hidden states must carry all relevant information from preceding steps. Together, this results\\nin the delta components deviating signiﬁcantly more from their respective input components for\\nVED relative to AED and AO (Fig. 2f). That is, since hidden states must hold the information of\\ninputs/outputs for multiple time steps, we cannot expect them to be well approximated by µE\\nt+χE\\nx\\nbecause, by deﬁnition, it is agnostic to the network’s inputs at any time other than t(and similarly for\\nµE\\ns+χE\\ny). As such, the temporal and input component decomposition gains us little insight into the\\ninner workings of the VED architecture. Additional details of the VED architecture dynamics are\\ndiscussed in Appendix B.6.\\nAdditional Tasks. In this section, we brieﬂy address how two additional synthetic tasks can be'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 5}, page_content='discussed in Appendix B.6.\\nAdditional Tasks. In this section, we brieﬂy address how two additional synthetic tasks can be\\nunderstood using the temporal and input component decomposition. First, consider a task identical to\\nthe one-to-one task, with the target sequence reversed in time, e.g. {B,A,C,A,D}→{ 4,1,3,1,2}.\\nFor this task, we expect an attention matrix that is anti-diagonal (i.e. it is nonzero for t=S+ 1−s).\\nFor the AED and AO networks trained on this task, we ﬁnd their temporal and input component\\nbehavior to be identical to the original one-to-one task with one exception: instead of the encoder and\\ndecoder temporal components following one another, we ﬁnd one trajectory is ﬂipped in such a way\\nas to yield an anti-diagonal attention matrix (Fig. 2g). That is, the last encoder temporal component\\nis aligned with the ﬁrst decoder temporal component and vice versa.\\nSecond, consider the task of sorting the input alphabetically, e.g. {B,C,A,D}→{ A,B,C,D}.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 5}, page_content='is aligned with the ﬁrst decoder temporal component and vice versa.\\nSecond, consider the task of sorting the input alphabetically, e.g. {B,C,A,D}→{ A,B,C,D}.\\nFor this example, we expect the network to learn an input-dependent attention matrix that correctly\\npermutes the input sequence. Since there is no longer a correlation between input and output sequence\\nlocation, the average attention matrix is very different from that of a random sample, and so we expect\\nthe temporal components to insigniﬁcantly contribute to the alignment. Indeed, we ﬁnd µD\\ns·µE\\ntto be\\nnegligible, and instead ∆hD\\ns·χE\\nxdominates the alignment values (Fig. 2h).\\n4 Beyond One-to-One Results\\nIn this section we analyze the dynamics of two tasks that have “close-to-diagonal” attention: (1)\\nwhat we refer to as the extended SCAN dataset and (2) translation between English and French\\nphrases. Since we found temporal/input component decomposition to provide little insight into VED'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 5}, page_content='what we refer to as the extended SCAN dataset and (2) translation between English and French\\nphrases. Since we found temporal/input component decomposition to provide little insight into VED\\ndynamics, our focus in this section will be on only the AED and AO architectures. For both tasks we\\nexplore below, parts of the picture we established on the one-to-one task continues to hold. However,\\n6'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 6}, page_content='Figure 4: Summary of features for AO trained on English to French translation. (a) Sample attention\\nmatrix. (b)The encoder (orange) and decoder (purple) temporal components, with a square and star marking the\\nﬁrst and last time step, respectively. Once again, quantities are projected onto the temporal component PCs. The\\ninset shows the attention matrix from the temporal components, i.e. the softmax of µD\\ns·µE\\nt.(c)The dot product\\nbetween the most common output word readouts and the most common input word input components, χE\\nx.\\nwe will see that in order to succeed at these tasks, both AO and AED must implement additional\\nmechanisms on top of the dynamics we saw for the one-to-one task.\\nExtended SCAN (eSCAN) is a modiﬁed version of the SCAN dataset (Lake & Baroni, 2018), in\\nwhich we randomly concatenate a subset of the phrases to form phrases of length 15to20(see\\nAppendix A.3 for details). The eSCAN tasks is close to one-to-one translation, but is augmented'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 6}, page_content='which we randomly concatenate a subset of the phrases to form phrases of length 15to20(see\\nAppendix A.3 for details). The eSCAN tasks is close to one-to-one translation, but is augmented\\nwith several additional rules that modify its structure. For example, a common sequence-to-sequence\\nstructure is that a pair of outputs can swap order relative to their corresponding inputs: the English\\nwords ‘green ﬁeld’ translate to ‘champ vert’ in French (with ‘ﬁeld’ ↔‘champ’ and ‘green’ ↔‘vert’).\\nThis behavior is present in eSCAN: when the input word ‘left’ follows a verb the output command\\nmust ﬁrst turn the respective direction and then perform said action (e.g. ‘run left’ →‘LTURN\\nRUN’).\\nThe AED and AO models both achieve ≥98% word accuracy on eSCAN. Looking at a sample\\nattention matrix of AED, we see consecutive words in the output phrase tend to attend to the same\\nencoder hidden states at the end of subphrases in the input phrase (Fig. 3a). Once again decomposing'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 6}, page_content='attention matrix of AED, we see consecutive words in the output phrase tend to attend to the same\\nencoder hidden states at the end of subphrases in the input phrase (Fig. 3a). Once again decomposing\\nthe AED network’s hidden states as in (2), we ﬁnd the temporal components of the encoder and\\ndecoder form curves that mirror one another, leading to an approximately diagonal attention matrix\\n(Fig. 3b). The delta components are signiﬁcantly less negligible for this task, as evidence by the fact\\nχE\\nx+ ∆hE\\ntaren’t nearly as clustered around their corresponding input component (Fig. 3c). As we\\nwill verify later, this is a direct result of the network’s use of recurrence, since now hidden states\\ncarry information about subphrases, rather than just individual words.\\nTraining the AO architecture on eSCAN, we also observe non-diagonal attention matrices, but in\\ngeneral their qualitative features differ from those of the AED architecture (Fig. 3d). Focusing on'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 6}, page_content='Training the AO architecture on eSCAN, we also observe non-diagonal attention matrices, but in\\ngeneral their qualitative features differ from those of the AED architecture (Fig. 3d). Focusing on\\nthe subphrase mapping ‘run twice’ →‘RUN RUN’, we see the network learns to attend to the word\\npreceding ‘twice’, since it can no longer rely on recurrence to carry said word’s identity forward.\\nOnce again, the temporal components of the encoder and decoder trace out paths that roughly follow\\none another (Fig. 3e). We see input-delta components cluster around their corresponding input\\ncomponents, indicating the delta components are small (Fig. 3f). Finally, we again see the readouts\\nof particular outputs align well with the input components of their corresponding input word.\\nEnglish to French Translation is another example of a nearly-diagonal task. We train the AED\\nand AO architectures on this natural language task using a subset of the para_crawl dataset Bañón'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 6}, page_content='English to French Translation is another example of a nearly-diagonal task. We train the AED\\nand AO architectures on this natural language task using a subset of the para_crawl dataset Bañón\\net al. (2020) consisting of over 30 million parallel sentences. To aid interpetation, we tokenize each\\nsentence at the word level and maintain a vocabulary of 30k words in each language; we train on\\nsentences of length up to 15 tokens.\\nSince English and French are syntactically similar with roughly consistent word ordering, the attention\\nmatrices are in general close to diagonal (Fig. 4a). Again, note the presence of features that require\\noff-diagonal attention, such as the ﬂipping of word ordering in the input/output phrases and multiple\\nwords in French mapping to a single English word. Using the decomposition of (2), the temporal\\ncomponents in both AED and AO continue to trace out similar curves (Fig. 4b). Notably, the'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 6}, page_content='words in French mapping to a single English word. Using the decomposition of (2), the temporal\\ncomponents in both AED and AO continue to trace out similar curves (Fig. 4b). Notably, the\\nalignment resulting from the temporal components is signiﬁcantly less diagonal, with the diagonal\\nbehavior clearest at the beginning of the phrase. Such behavior makes sense: the presence of off-\\ndiagonal structure means, on average, translation pairs become increasingly offset the further one\\n7'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 7}, page_content='Figure 5: Temporal and input component features. In the ﬁrst three plots, the data shown in red, blue, and\\ngreen corresponds to networks trained on the one-to-one, eSCAN, and English to French translation tasks,\\nrespectively. (a)Breakdown of the nine terms that contribute to the largest alignment scores (see (3)) averaged\\nacross the entire decoder sequence for each task/architecture combination (see Appendix A.2 for details). For\\neach bar, from top to bottom, the alignment contributions from µD\\ns·µE\\nt(dark), µD\\ns·χE\\nx+µD\\ns·∆hE\\nt(medium),\\nand the remaining six terms (light). (b)For the AO architecture, the dot product of the temporal components,\\nµD\\ns·µE\\nt, as a function of the offset, t−s, shown at different decoder times. Each offset is plotted from [−5,5]\\nand the dotted lines show the theoretical prediction for maximum offset as a function of decoder time, s. Plots\\nfor the AED architecture are qualitatively similar. (c)For all hidden states corresponding to an input word, the'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 7}, page_content='for the AED architecture are qualitatively similar. (c)For all hidden states corresponding to an input word, the\\nratio of variance of hE\\nt−µE\\nttohE\\nt.(d)For AO trained on eSCAN, the dot product of input components, χE\\nx,\\nwith each of the readouts (AED is qualitatively similar).\\nmoves into a phrase. With offsets that increasingly vary from phrase to phrase, the network must rely\\nless on temporal component alignments, which by deﬁnition are independent of the inputs. Finally,\\nwe see that the the dot product between the input components and the readout vectors implement\\nthe translation dictionary, just as it did for the one-to-one task (Fig. 4c, see below for additional\\ndiscussion).\\n4.1 A Closer Look at Model Features\\nAs expected, both the AED and AO architectures have more nuanced attention mechanisms when\\ntrained on eSCAN and translation. In this section, we investigate a few of their features in detail.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 7}, page_content='As expected, both the AED and AO architectures have more nuanced attention mechanisms when\\ntrained on eSCAN and translation. In this section, we investigate a few of their features in detail.\\nAlignment Approximation. Recall that for the one-to-one task, we found the alignment scores\\ncould be well approximated by ast≈µD\\ns·µE\\nt, which was agnostic to the details of the input\\nsequence. For eSCAN, the µD\\ns·µE\\ntterm is still largely dominant, capturing >77% ofastin\\nthe AED and AO networks (Fig. 5a). A better approximation for the alignment scores is ast≈\\nµD\\ns·µE\\nt+µD\\ns·χE\\nx+µD\\ns·∆hE\\nt, i.e. we include two additional terms on top of what was used for\\none-to-one. Since χE\\nxand∆hE\\ntare dependent upon the input sequence, this means the alignment has\\nnon-trivial input dependence, as we would expect. In both architectures, we ﬁnd this approximation\\ncaptures>87% of the top alignment scores. For translation, we see the term µD\\ns·µE\\ntmakes up a'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 7}, page_content='non-trivial input dependence, as we would expect. In both architectures, we ﬁnd this approximation\\ncaptures>87% of the top alignment scores. For translation, we see the term µD\\ns·µE\\ntmakes up a\\nsigniﬁcantly smaller portion of the alignment scores, and in general we ﬁnd none of the nine terms in\\n(3)dominate above the rest (Fig. 5a). However, at early times in the AED architecture, we again see\\nµD\\ns·µE\\ntis the largest contribution to the alignment. As mentioned above, this matches our intuition\\nthat words at the start of the encoder/decoder phrase have a smaller offset from one another than later\\nin the phrase, so the network can rely more on temporal components to determine attention.\\nTemporal Component Offset. For the one-to-one task, the input sequence length was always equal\\nto the output sequence length, so the temporal components were always peaked at s=t(Fig. 5b). In\\neSCAN, the input word ‘and’ has no corresponding output, which has a non-trivial effect on how'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 7}, page_content='to the output sequence length, so the temporal components were always peaked at s=t(Fig. 5b). In\\neSCAN, the input word ‘and’ has no corresponding output, which has a non-trivial effect on how\\nthe network attends since its appearance means later words in input phrase are offset from their\\ncorresponding output word. This effect also compounds with multiple occurrences of ‘and’ in the\\ninput. The AED and AO networks learn to handle such behavior by biasing the temporal component\\ndot product, µD\\ns·µE\\nt, the dominant alignment contribution, to be larger for time steps tfurther along\\nin the encoder phrase, i.e. t>s (Fig. 5b). It is possible to compute the average offset of input and\\noutput words in eSCAN training set, and we see the maximum of µD\\ns·µE\\ntfollow this estimate quite\\nwell. Similarly, in our set of English to French translation phrases, we ﬁnd the French phrases to be\\non average∼20% longer than their English counterparts. This results in the maximum of µD\\ns·µE\\nt'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 7}, page_content='well. Similarly, in our set of English to French translation phrases, we ﬁnd the French phrases to be\\non average∼20% longer than their English counterparts. This results in the maximum of µD\\ns·µE\\nt\\nto gradually move toward t<s , e.g. on average the decoder attends to earlier times in the encoder\\n(Fig. 5b). Additionally, note the temporal dot product falls off signiﬁcantly slower as a function of\\noffset for later time steps, indicating the drop off for non-diagonal alignments is smaller and thus it is\\neasier for the network to off-diagonally attend.\\n8'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 8}, page_content='Figure 6: How AO and AED networks implement off-diagonal attention in the eSCAN dataset. (a) For\\nAED, the input-delta components for various words and subphrases. (b)For AO, the alignment values, ast, are\\nshown in black when the input word ‘twice’ is at t=s. Three contributions to the alignment, µD\\ns·µE\\nt(gold),\\nµD\\ns·χE\\nx+µD\\ns·∆hE\\nt(pink), and ast−µD\\ns·hE\\nt(grey) are also plotted. To keep the offset between ‘twice’ and\\nthe output location of the repeated word constant, this plot was generated on a subset of eSCAN with T=S,\\nbut we observe the same qualitative features when T≥S.(c)The dot product between χE\\nx+ ∆hE\\ntand the\\ndecoder’s temporal component, µD\\ns, fort=s.(d)How the dot product of χE\\nx+ ∆hE\\ntandµD\\nschanges as a\\nfunction of their offset, t−s, for a few select input words. The vertical gray slice represents the data in (c) and\\nthe input word colors are the same.\\nWord Variance. The encoder hidden states in the one-to-one task had a negligible delta component,'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 8}, page_content='the input word colors are the same.\\nWord Variance. The encoder hidden states in the one-to-one task had a negligible delta component,\\nso the hidden states could be approximated as hE\\nt≈µE\\nt+χE\\nx. By deﬁnition, χE\\nxis constant for a\\ngiven input word, so the variance in the hidden states corresponding to a given input word is primarily\\ncontained in the temporal component (Fig. 5c). Since the temporal component is input-independent,\\nthis led to a clear understanding of how all of a network’s hidden states evolve with time and input.\\nIn the AED and AO architectures trained on eSCAN, we ﬁnd the variance of the input word’s hidden\\nstates drops by 90% and95% when the temporal component is subtracted out, respectively (Fig. 5c).\\nMeanwhile, in translation, we ﬁnd the variance only drops by 8%and25% for the AED and AO\\narchitectures, indicating there is signiﬁcant variance in the hidden states beyond the average temporal\\nevolution and thus more intricate dynamics.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 8}, page_content='architectures, indicating there is signiﬁcant variance in the hidden states beyond the average temporal\\nevolution and thus more intricate dynamics.\\nInput/Readout Alignment. Lastly, recall we saw that in the one-to-one case the input components’\\nalignment with readouts implemented the translation dictionary (Figs. 2b, d). For eSCAN, the dot\\nproduct of a given readout is again largest with the input component of its corresponding input word,\\ne.g. the readout corresponding to ‘RUN’ is maximal for the input component of ‘run’ (Fig. 5d).\\nNotably, words that produce no corresponding output such as ‘and’ and ‘twice’ are not the maximal\\nin alignment with any readout vector. Similarly, for translation, we see the French-word readouts\\nhave the largest dot product their translated English words (Fig. 4c). For example, the readouts for\\nthe words ‘la’, ‘le’, and ‘les’, which are the gendered French equivalents of ‘the’, all have maximal\\nalignments with χE\\nthe.\\n4.2 A Closer Look at Dynamics'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 8}, page_content='the words ‘la’, ‘le’, and ‘les’, which are the gendered French equivalents of ‘the’, all have maximal\\nalignments with χE\\nthe.\\n4.2 A Closer Look at Dynamics\\nIn this section, we leverage the temporal and input component decomposition to take a closer look at\\nhow networks trained on the eSCAN dataset implement particular off-diagonal attentions. Many of\\nthe sequence translation structures in eSCAN are seen in realistic datasets, so we this analysis will\\ngive clues toward understanding the behavior of more complicated sequence-to-sequence tasks.\\nA common structure in sequence-to-sequence tasks is when an output word is modiﬁed by the words\\npreceding it. For example, the phrases ‘we run‘ and ‘they run’ translate to ‘nous courrons’ and ‘ils\\ncourent’ in French, respectively (with the second word in each the translation of ‘run’). We can study\\nthis phenomenon in eSCAN since the word ‘twice’ tells the network to repeat the command just'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 8}, page_content='courent’ in French, respectively (with the second word in each the translation of ‘run’). We can study\\nthis phenomenon in eSCAN since the word ‘twice’ tells the network to repeat the command just\\nissued two times, e.g. ‘run twice’ outputs to ‘RUN RUN’. Hence, the output corresponding to the\\ninput ‘twice’ changes based on other words in the phrase.\\nSince an AED network has recurrence, when it sees the word ‘twice’ it can know what verb preceded\\nit. Plotting input-delta components, we see the RNN outputs ‘twice’ hidden states in three separate\\nclusters separated by the preceding word (Fig. 6a). Thus for an occurrence of ‘twice’ at time step t,\\nwe have χE\\ntwice+ ∆hE\\nt≈χE\\nverb+ ∆hE\\nt−1. For example, this means the AED learns to read in ‘run\\ntwice’ approximately the same as ‘run run’. This is an example of the network learning context.\\n9'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 9}, page_content='AO has no recurrence, so it can’t know which word was output before ‘twice’. Hence, unlike the\\nAED case, all occurrences of ‘twice’ are the same input-delta component cluster regardless of what\\nword preceded it. Instead, it has to rely on attending to the word that modiﬁes the output, which in\\nthis case is simply the preceding word (Fig. 3d). As mentioned in Sec. 4.1, for the eSCAN task we\\nﬁnd the alignment to be well approximated by ast≈µD\\ns·hE\\nt. When the word ’twice’ appears in the\\ninput phrase, we ﬁnd µD\\ns·χE\\ntwice+µD\\ns·∆hE\\nt<0fors=t(Fig. 6b). This decreases the value of\\nthe alignment as,s, and so the decoder instead attends to the time step with the second largest value\\nofµD\\ns·µE\\nt, which the network has learned to be t=s−1. Hence,as,s−1is the largest alignment,\\ncorresponding to the time step before ‘twice’ with the verb the network needs to output again. Unlike\\nthe one-to-one case, the encoder input-delta and the decoder temporal components are no longer'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 9}, page_content='corresponding to the time step before ‘twice’ with the verb the network needs to output again. Unlike\\nthe one-to-one case, the encoder input-delta and the decoder temporal components are no longer\\napproximately orthogonal to one another (Fig. 6c). In the case of ‘twice’, χE\\ntwice+ ∆hE\\ntis partially\\nantialigned with the temporal component, yielding a negative dot product.\\nThis mechanism generalizes beyond the word ’twice’: in eSCAN we see input-delta components of\\nseveral input words are no longer orthogonal to the decoder’s temporal component (Fig. 6c). Like\\n‘twice’, the dot product of the input-delta component for a given word with its corresponding temporal\\ncomponent determines how much its alignment score is increased/decreased. For example, we see\\nχE\\nand+ ∆hE\\nthas a negative dot product with the temporal component, meaning it leans away from its\\ncorresponding temporal component. Again, this make sense from eSCAN task: the word ‘and’ has'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 9}, page_content='χE\\nand+ ∆hE\\nthas a negative dot product with the temporal component, meaning it leans away from its\\ncorresponding temporal component. Again, this make sense from eSCAN task: the word ‘and’ has\\nno corresponding output, hence it never wants to be attended to by the decoder.\\nPerhaps contradictory to expectation, χE\\nleft+ ∆hE\\nthas a negative dot product with the temporal\\ncomponent. However, note that the alignment of χE\\nx+ ∆hE\\ntwith the hD\\nsis dependent on both tands.\\nWe plot the dot products of χE\\nx+ ∆hE\\ntandhD\\nsas a function of their offset, deﬁned to be the t−s\\n(Fig. 6d). Notably, χE\\nleft+ ∆hE\\nthas a larger dot product for larger offsets, meaning it increases its\\nalignment when t>s . This makes sense from the point of view that the word ‘left’ is always further\\nalong in the input phrase than its corresponding output ‘LTURN’, and this offset is only compounded\\nby the presence of the word ‘and’. Thus, the word ‘left’ only wants to get noticed if it is ahead of the'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 9}, page_content='by the presence of the word ‘and’. Thus, the word ‘left’ only wants to get noticed if it is ahead of the\\ncorresponding decoder time step, otherwise it hides. Additionally, the words ‘and‘ and ‘twice‘ have\\nlarge negative dot products for all offsets, since they never want to be the subject of attention.\\n5 Discussion\\nIn this work, we studied the hidden state dynamics of sequence-to-sequence tasks in architectures\\nwith recurrence and attention. We proposed a decomposition of the hidden states into parts that\\nare input- and time-independent and showed when such a decomposition aids in understanding the\\nbehavior of encoder-decoder networks.\\nAlthough we have started by analyzing translation tasks, it would be interesting to understand how said\\ndecomposition works on different sequence-to-sequence tasks, such as speech-to-text. Additionally,\\nwith our focus on the simplest encoder-decoder architectures, it is important to investigate how'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 9}, page_content='decomposition works on different sequence-to-sequence tasks, such as speech-to-text. Additionally,\\nwith our focus on the simplest encoder-decoder architectures, it is important to investigate how\\nmuch the observed dynamics generalize to more complicated network setups, such as networks with\\nbidirectional RNNs or multiheaded and self-attention mechanisms. Our analysis of the attention-\\nonly architecture, which bears resemblance to the transformer architecture, suggests that a similar\\ndynamical behavior may also hold for the Transformer, hinting at the working mechanisms behind\\nthis popular non-recurrent architecture.\\nAcknowledgments and Disclosure of Funding\\nWe thank Ankush Garg for collaboration during the early part of this work. None of the authors\\nreceive third-party funding/support during the 36 months prior to this submission or had competing\\ninterests.\\nReferences\\nAitken, K., Ramasesh, V . V ., Garg, A., Cao, Y ., Sussillo, D., and Maheswaranathan, N. The geometry'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 9}, page_content='interests.\\nReferences\\nAitken, K., Ramasesh, V . V ., Garg, A., Cao, Y ., Sussillo, D., and Maheswaranathan, N. The geometry\\nof integration in text classiﬁcation rnns. arXiv preprint arXiv:2010.15114 , 2020.\\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine translation by jointly learning to align and\\ntranslate. arXiv preprint arXiv:1409.0473 , 2014.\\n10'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 10}, page_content='Bañón, M., Chen, P., Haddow, B., Heaﬁeld, K., Hoang, H., Esplà-Gomis, M., Forcada, M. L.,\\nKamran, A., Kirefu, F., Koehn, P., Ortiz Rojas, S., Pla Sempere, L., Ramírez-Sánchez, G.,\\nSarrías, E., Strelec, M., Thompson, B., Waites, W., Wiggins, D., and Zaragoza, J. ParaCrawl:\\nWeb-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the\\nAssociation for Computational Linguistics , pp. 4555–4567, Online, July 2020. Association for\\nComputational Linguistics. doi: 10.18653/v1/2020.acl-main.417. URL https://www.aclweb.\\norg/anthology/2020.acl-main.417 .\\nBastings, J. and Filippova, K. The elephant in the interpretability room: Why use attention as\\nexplanation when we have saliency methods?, 2020.\\nChan, W., Jaitly, N., Le, Q. V ., and Vinyals, O. Listen, attend and spell, 2015.\\nChefer, H., Gur, S., and Wolf, L. Transformer interpretability beyond attention visualization, 2020.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 10}, page_content='Chan, W., Jaitly, N., Le, Q. V ., and Vinyals, O. Listen, attend and spell, 2015.\\nChefer, H., Gur, S., and Wolf, L. Transformer interpretability beyond attention visualization, 2020.\\nChiu, C., Sainath, T. N., Wu, Y ., Prabhavalkar, R., Nguyen, P., Chen, Z., Kannan, A., Weiss,\\nR. J., Rao, K., Gonina, E., Jaitly, N., Li, B., Chorowski, J., and Bacchiani, M. State-of-the-art\\nspeech recognition with sequence-to-sequence models. In 2018 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP) , pp. 4774–4778, 2018. doi: 10.1109/ICASSP.\\n2018.8462105.\\nCho, K., van Merrienboer, B., Gülçehre, Ç., Bougares, F., Schwenk, H., and Bengio, Y . Learning\\nphrase representations using RNN encoder-decoder for statistical machine translation. CoRR ,\\nabs/1406.1078, 2014.\\nCollins, J., Sohl-Dickstein, J., and Sussillo, D. Capacity and trainability in recurrent neural networks,\\n2016.\\nDing, S., Xu, H., and Koehn, P. Saliency-driven word alignment interpretation for neural machine'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 10}, page_content='2016.\\nDing, S., Xu, H., and Koehn, P. Saliency-driven word alignment interpretation for neural machine\\ntranslation. arXiv preprint arXiv:1906.10282 , 2019.\\nGhader, H. and Monz, C. What does attention in neural machine translation pay attention to? arXiv\\npreprint arXiv:1710.03348 , 2017.\\nHochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation , 9(8):1735–1780,\\n1997.\\nJain, S. and Wallace, B. C. Attention is not explanation, 2019.\\nKingma, D. and Ba, J. Adam: A method for stochastic optimization. International Conference on\\nLearning Representations , 12 2014.\\nLake, B. and Baroni, M. Generalization without systematicity: On the compositional skills of\\nsequence-to-sequence recurrent networks. In International Conference on Machine Learning , pp.\\n2873–2882. PMLR, 2018.\\nLuong, M.-T., Pham, H., and Manning, C. D. Effective approaches to attention-based neural machine\\ntranslation. arXiv preprint arXiv:1508.04025 , 2015.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 10}, page_content='2873–2882. PMLR, 2018.\\nLuong, M.-T., Pham, H., and Manning, C. D. Effective approaches to attention-based neural machine\\ntranslation. arXiv preprint arXiv:1508.04025 , 2015.\\nMaheswaranathan, N. and Sussillo, D. How recurrent networks implement contextual processing in\\nsentiment analysis. arXiv preprint arXiv:2004.08013 , 2020.\\nMaheswaranathan, N., Williams, A., Golub, M., Ganguli, S., and Sussillo, D. Reverse engineering\\nrecurrent networks for sentiment classiﬁcation reveals line attractor dynamics. In Advances in\\nNeural Information Processing Systems 32 , pp. 15696–15705. Curran Associates, Inc., 2019.\\nPrabhavalkar, R., Rao, K., Sainath, T., Li, B., Johnson, L., and Jaitly, N. A comparison of sequence-\\nto-sequence models for speech recognition. 2017. URL http://www.isca-speech.org/\\narchive/Interspeech_2017/pdfs/0233.PDF .\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 10}, page_content='archive/Interspeech_2017/pdfs/0233.PDF .\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J.\\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2020.\\nSerrano, S. and Smith, N. A. Is attention interpretable? arXiv preprint arXiv:1906.03731 , 2019.\\n11'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 11}, page_content='Sutskever, I., Vinyals, O., and Le, Q. V . Sequence to sequence learning with neural networks. arXiv\\npreprint arXiv:1409.3215 , 2014.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\\nPolosukhin, I. Attention is all you need. In Advances in neural information processing systems , pp.\\n5998–6008, 2017.\\nWiegreffe, S. and Pinter, Y . Attention is not not explanation, 2019.\\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M., Macherey, W., Krikun, M., Cao, Y ., Gao, Q.,\\nand et al., K. M. Google’s neural machine translation system: Bridging the gap between human\\nand machine translation, 2016.\\nA Additional Details\\nFigure 7: Comparison of the three primary architectures used in this work and the relation between\\nthem. The three architectures are vanilla encoder-decoder (VED), encoder-decoder with attention (AED), and\\nattention only (AO). The encoder RNNs, decoder RNNs, and linear readout layer are showing in orange, purple,'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 11}, page_content='attention only (AO). The encoder RNNs, decoder RNNs, and linear readout layer are showing in orange, purple,\\nand green, respectively. Recurrent connections between RNNs are shown in blue, attention-based connections\\nand computational blocks are shown in gold. For AO, the grey circles represent locations where positional\\nencoding is added to the inputs. Note AED’s linear readout layer takes in both the context vector from attention\\nas well as the decoder’s output.\\nIn this section we provide additional details regarding the architectures, temporal-input component\\ndecomposition, datasets, RNNs, and training used in this work.\\nAs a reminder, the encoder and decoder hidden states are denoted by hE\\ntandhD\\ns, respectively, while\\ninputs to the encoder and decoder are denoted by xE\\ntandxD\\ns.\\nA.1 Architectures\\nA summary of the three architectures we focus on—vanilla encoder-decoder, encoder-decoder with'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 11}, page_content='s, respectively, while\\ninputs to the encoder and decoder are denoted by xE\\ntandxD\\ns.\\nA.1 Architectures\\nA summary of the three architectures we focus on—vanilla encoder-decoder, encoder-decoder with\\nattention, and attention only—in this work is shown in Fig. 7. Intuitively, the architectures are related\\nas follows: AED has attention and recurrence, VED and AO are the same as AED with the attention\\nand recurrence removed, respectively (for AO, we also add positional encoding).\\nVanilla Encoder Decoder\\nThe encoder and decoder update expression are\\nhE\\nt=FE(hE\\nt−1,xE\\nt), hD\\ns=FD(hD\\ns−1,xD\\ns), (4)\\nrespectively. Here, FDandFEare the functions that implement the hidden state updates, which in\\nthis work are each one of three modern RNN architectures: LSTMs (Hochreiter & Schmidhuber,\\n1997), GRUs (Cho et al., 2014), or UGRNNs (Collins et al., 2016). The ﬁnal encoder hidden state is\\nthe decoder’s initial hidden state, so that hD\\n0=hE\\nT. The decoder hidden states are passed through a'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 11}, page_content='the decoder’s initial hidden state, so that hD\\n0=hE\\nT. The decoder hidden states are passed through a\\nlinear layer to get the output logits at each time step, ys=WhD\\ns+b, with the following decoder\\ninput, xD\\ns+1, determined by the word corresponding to the maximum output logit, argmax (ys).\\n12'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 12}, page_content='Encoder Decoder with Attention\\nThe encoder-decoder with attention architecture is identical to the VED architecture above with a\\nsimple attention mechanism added (Bahdanau et al., 2014; Luong et al., 2015). For time step sof the\\ndecoder, we compute a context vector cs, a weighted sum of encoder hidden states,\\ncs:=T∑\\nt=1αsthE\\nt, αst:=east\\n∑T\\nt′=1east′. (5)\\nHere, αt:=softmax (a1t,...,aSt)is thet-th column of the attention matrix andast:=hD\\ns·hE\\nt\\nthealignment between a given decoder and encoder hidden state. Furthermore, the outputs are now\\ndetermined by passing both the decoder hidden state and the context vector through the linear layer,\\ni.e.ys=W[hD\\ns,cs] +b, where [·,·]denotes concatenation Luong et al. (2015).\\nAttention Only\\nAttention only is identical to the AED network above, but simply eliminates the recurrent information\\npassed from one RNN cell to the next. Since this eliminates any sense of temporal ordering in the'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 12}, page_content='Attention only is identical to the AED network above, but simply eliminates the recurrent information\\npassed from one RNN cell to the next. Since this eliminates any sense of temporal ordering in the\\nsequences, we also add ﬁxed positional encoding vectors (Vaswani et al., 2017), pE\\ntandpD\\nsto the\\nencoder and decoder inputs. Together, this means the hidden state update expressions are now\\nhE\\nt=FE(0,xE\\nt+pE\\nt), hD\\ns=FD(0,xD\\ns+pD\\ns). (6)\\nNote the RNN functions FEandFDsimply act as feedforward networks in this setting. Lastly, the\\noutput logits are now determined solely from the context vector, ys=Wcs+b.\\nAlthough using gated RNNs cells as feedforward networks is fairly non-standard, our primary\\nmotivation is to keep the AED and AO architectures as similar as possible in order to isolate the\\ndifferences that arise from recurrence and positional encoding. Below we discuss a non-gated\\nfeedforward variant that we also brieﬂy investigate.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 12}, page_content='differences that arise from recurrence and positional encoding. Below we discuss a non-gated\\nfeedforward variant that we also brieﬂy investigate.\\nNote the elimination of recurrence means the entire encoder hidden states sequence can be computed\\nin parallel. This architecture is meant to be a simpliﬁed model of a Transformer (Vaswani et al.,\\n2017). The hidden states output by the RNN simultaneous ﬁll the role of the usual keys, queries, and\\nvalues of a Transformer. Additionally, there is no self-attention mechanism, only a single “head” of\\nattention, and no residual connections.\\nPositional Encoding For ad-dimensional embedding dimension at time t, we add the vector pE\\nt\\nwithi= 0,...,d−1components:\\npE\\nt,i={sin(t\\nτi/d)\\nieven\\ncos(t\\nτ(i−1)/d)\\niodd(7)\\nwithτsome temporal scale that should be related to the phrase length. This is the same positional\\nencoding used in Vaswani et al. (2017), and we use the same encoding for both the encoder ( pE\\nt) and\\ndecoder ( pD\\nt).'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 12}, page_content='encoding used in Vaswani et al. (2017), and we use the same encoding for both the encoder ( pE\\nt) and\\ndecoder ( pD\\nt).\\nNon-Gated Variant As mentioned above, we use a gated-RNN with its recurrence cut as a feed-\\nforward network. Since this is fairly non-standard, we also verify some of our results on non-gated\\nfeedforward architectures. The non-gated variant of AO uses a the following hidden-state updates\\nhE\\nt=F′\\nE(xE\\nt) := tanh(\\nWExE\\nt+bE)\\n, (8a)\\nhD\\ns=F′\\nD(xD\\ns) := tanh(\\nWDxD\\ns+bD)\\n, (8b)\\nwith tanh acting pointwise. This architecture is identical to the version of AO used above, but the\\nhidden state updates are now\\nhE\\nt=F′\\nE(xE\\nt+pE\\nt), hD\\ns=F′\\nD(xD\\ns+pD\\ns). (9)\\nBelow, we show that we ﬁnd the qualitative results of this network are the same as the gated\\nversion, and thus our results do not seem to be dependent upon the gating mechanisms present in the\\nfeedforward networks of the AO architecture.\\n13'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 13}, page_content='A.2 Temporal and Input Components\\nWe deﬁne the temporal components to be the average hidden state at a given time step. In practice,\\nwe estimate such averages using a test set of size M, so that the temporal components are given by\\nµE\\nt≈∑M\\nα=11≤EoS,αhE\\nt,α∑M\\nβ=11≤EoS,β, (10a)\\nµD\\ns≈∑M\\nα=11≤EoS,αhD\\ns,α∑M\\nβ=11≤EoS,β, (10b)\\nwithhE\\nt,αthe encoder hidden state of the αth sample and 1≤EoS,αis a mask that is zero if the αth\\nsample is beyond the end of sentence. Next, we deﬁne the encoder input components to be the average\\nofhE\\nt−µE\\ntfor all hidden states that immediately follow a given input word (and similarly for the\\ndecoder input components). Once again, we estimate the input components using a test set of size M,\\nχE(xt,α)≈∑M\\nβ=1∑T\\nt′=11xt,α,xt′,β(\\nhE\\nt′,β−µE\\nt′)\\n∑M\\nγ=1∑T\\nt′′=11xt,α,xt′′,γ, (11a)\\nχD(xs,α)≈∑M\\nβ=1∑S\\ns′=11xs,α,xs′,β(\\nhD\\ns′,β−µD\\ns′)\\n∑M\\nγ=1∑S\\ns′′=11xs,α,xs′′,γ, (11b)\\nwhere 1xt,α,xt′,βis a mask that is zero if xt,α̸=xt′,βand we have temporally suppressed the'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 13}, page_content='χD(xs,α)≈∑M\\nβ=1∑S\\ns′=11xs,α,xs′,β(\\nhD\\ns′,β−µD\\ns′)\\n∑M\\nγ=1∑S\\ns′′=11xs,α,xs′′,γ, (11b)\\nwhere 1xt,α,xt′,βis a mask that is zero if xt,α̸=xt′,βand we have temporally suppressed the\\nsuperscripts on the inputs for brevity. With the above deﬁnitions, we can decompose encoder and\\ndecoder hidden states resulting from the αth sample as\\nhE\\nt,α=µE\\nt+χE(\\nxE\\nt,α)\\n+ ∆hE\\nt,α, (12a)\\nhD\\ns,α=µD\\ns+χD(\\nxD\\ns,α)\\n+ ∆hD\\ns,α, (12b)\\nwith the delta components deﬁned to be whatever is leftover in the hidden state after subtracting out\\nthe temporal and input components,\\n∆hE\\nt,α:=hE\\nt,α−µE\\nt−χE(\\nxE\\nt,α)\\n, (13a)\\n∆hD\\ns,α:=hD\\ns,α−µD\\ns−χD(\\nxD\\ns,α)\\n. (13b)\\nIn the main text we use the shorthand χE\\nx=χE(\\nxE\\nt,α)\\nandχD\\ny=χD(\\nxD\\ns,α)\\n(since for the decoder,\\nthe previous time step’s output, ys−1is passed as the next input). We will often suppress the batch\\nindex, so altogether the decomposition is written in the main text as\\nhE\\nt=µE\\nt+χE\\nx+ ∆hE\\nt, (14a)\\nhD\\ns=µD\\ns+χD\\ny+ ∆hD\\ns. (14b)'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 13}, page_content='index, so altogether the decomposition is written in the main text as\\nhE\\nt=µE\\nt+χE\\nx+ ∆hE\\nt, (14a)\\nhD\\ns=µD\\ns+χD\\ny+ ∆hD\\ns. (14b)\\nThe intuition behind this decomposition is that it is an attempt to isolate the temporal and input\\nbehavior of the network’s hidden state updates FEandFD. This partially motivated by the fact that, if\\nFEwere linear, then the encoder hidden state update for AO could be written as\\nhE\\nt=FE(0,xE\\nt+pE\\nt) =FE(0,xE\\nt) +FE(0,pE\\nt). (15)\\nNotably, the ﬁrst term is only dependent upon the input and the second term is only dependent upon\\nthe sequence index (through the positional encoding). In this case, the temporal and input component\\nwould exactly capture the time and input dependence of the hidden states, respectively. Of course, FE\\nis not in general linear, but we still ﬁnd such a decomposition useful for interpretation.\\nAlignment Using the above decomposition, we can write the alignment as a sum of nine terms:\\nast=(\\nµD\\ns+χD\\ny+ ∆hD\\ns)\\n·(\\nµE\\nt+χE\\nx+ ∆hE\\nt)\\n=µD\\ns·µE'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 13}, page_content='Alignment Using the above decomposition, we can write the alignment as a sum of nine terms:\\nast=(\\nµD\\ns+χD\\ny+ ∆hD\\ns)\\n·(\\nµE\\nt+χE\\nx+ ∆hE\\nt)\\n=µD\\ns·µE\\nt+µD\\ns·χE\\nx+µD\\ns·∆hE\\nt\\n+χD\\ny·µE\\nt+χD\\ny·χE\\nx+χD\\ny·∆hE\\nt\\n+ ∆hD\\ns·µE\\nt+ ∆hD\\ns·χE\\nx+ ∆hD\\ns·∆hE\\nt (16a)\\n:=9∑\\nI=1a(I)\\nst, (16b)\\n14'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 14}, page_content='Input Phrase Output Phrase\\nrun⟨.⟩jump⟨.⟩walk⟨.⟩RUN⟨.⟩JUMP⟨.⟩WALK⟨.⟩\\nrun left⟨.⟩ LTURN RUN⟨.⟩\\nrun twice⟨.⟩jump⟨.⟩ RUN RUN⟨.⟩JUMP⟨.⟩\\nrun and jump⟨.⟩ RUN JUMP⟨.⟩\\nTable 1: Extended SCAN example phrases.\\nwitha(I)\\nstforI= 1,..., 9deﬁned as the nine terms which sequentially appear after the second\\nequality (i.e. a(1)\\nst:=µD\\ns·µE\\nt,a(2)\\nst:=µD\\ns·χE\\nx, and so on).\\nIn the main text, we measure the breakdown of the alignment scores from each of the nine terms.\\nDeﬁne the contributions from one of the nine terms as\\nA(I)\\nst:=⏐⏐⏐a(I)\\nst⏐⏐⏐\\n∑9\\nJ=1⏐⏐⏐a(J)\\nst⏐⏐⏐, (17)\\nwhere the absolute values are necessary because contributions to the dot product alignment can be\\npositive or negative.\\nA.3 Datasets\\nOne-to-One Dataset This is a simple sequence to sequence task consisting variable length phrases\\nwith input and output words that are in one-to-one correspondence. At each time step, a word from a\\nword bank of size Nis randomly chosen (uniformly), and as such there is no correlation between'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 14}, page_content='with input and output words that are in one-to-one correspondence. At each time step, a word from a\\nword bank of size Nis randomly chosen (uniformly), and as such there is no correlation between\\nwords at different time steps. The length of a given input phrase is predetermined and also drawn a\\nuniform distribution. After an input phrase is generated, the corresponding output phrase is created by\\nindividually translating each word. All input words translate to a unique output word and translations\\nare solely dependent upon the input word. An example one-to-one dataset would be converting a\\nsequence of letters to their corresponding position in the alphabet, {B,A,C,A,A}→{ 2,1,3,1,1}.\\nNote the task of simply repeating the input phrase as the output is also one-to-one.\\nDue to the small vocabulary size, one-hot encoding is used for input phrases. Train and test sets are\\ngenerated dynamically.\\nExtended SCAN Extended SCAN (eSCAN) is a modiﬁed version of the SCAN dataset Lake &'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 14}, page_content='generated dynamically.\\nExtended SCAN Extended SCAN (eSCAN) is a modiﬁed version of the SCAN dataset Lake &\\nBaroni (2018). The SCAN dataset is a sequence-to-sequence task consisting of translating simple\\ninput commands into output actions. SCAN consists of roughly 20,000 phrases, with a maximum\\ninput and output phrase lengths of 9 and 49, respectively. A few relevant example phrases of eSCAN\\nare shown in Table 1.\\nThe eSCAN dataset modiﬁes SCAN in two ways:\\n1.It takes only a subset of SCAN in which input phrases solely consist of a chosen subset\\nof SCAN words. This allows us to isolate particular behaviors present in SCAN as well\\nas eliminate certain word combinations that would require far-from diagonal attention\\nmechanisms (e.g. it allows us to avoid the input subphrase ‘run around thrice’ that yields an\\noutput subphrase of length 24).\\n2.After a subset of SCAN phrases has been chosen, phrases are randomly drawn (uniformly)'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 14}, page_content='output subphrase of length 24).\\n2.After a subset of SCAN phrases has been chosen, phrases are randomly drawn (uniformly)\\nand concatenated together until a phrase of the desired length range is created. Individual\\nSCAN phrases are separated by a special word token (a period). This allows us to generate\\nphrases that are much longer than the phrases encountered in the SCAN dataset and also\\ncontrol the variance of phrase lengths.\\nThe eSCAN dataset allows us to gradually increase a phrase’s complexity while having control over\\nphrase lengths. At its simplest, eSCAN can also be one-to-one if one restricts to input phrases with\\nsome combination of the words ‘run’, ‘walk’, ‘jump’, and ‘look’.\\nThroughout the main text, we use the subset of SCAN that contains the words ‘run’, ‘walk’, ‘jump’,\\n‘and’, ‘left’, and ‘twice’ with lengths ranging from 10 to 15. Furthermore, we omit combinations that\\n15'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 15}, page_content='require a composition of rules, e.g. the phrase ‘jump left twice’ that requires the network to both\\nunderstand the reversing of ‘jump left’ and the repetition of ‘twice’. Although it would be interesting\\nto study if and how the network learns such compositions, we leave such studies for future work.\\nThis results in 90 distinct subphrases prior to concatenation, with a maximum input and output length\\nof 5 and 4, respectively. Post concatenation, there are over a million distinct phrases for phrases of\\nlength 10 to 15. Once again, one-hot encoding is used for the input and output phrases and train/test\\nsets are generated dynamically.\\nTranslation Dataset Our natural-language translation dataset is the ParaCrawl Corpus, or Web\\nScale Parallel Corpora for European Languages (Bañón et al., 2020); we train models to translate\\nbetween English and French, using the release of the dataset available in TensorFlow Datasets. This'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 15}, page_content='Scale Parallel Corpora for European Languages (Bañón et al., 2020); we train models to translate\\nbetween English and French, using the release of the dataset available in TensorFlow Datasets. This\\nrelease of ParaCrawl features 31,374,161 parallel English/French sentences; as we train our models\\nfor 30,000 steps using a batch size of 64, we do not encounter all examples during training.\\nTo aid interpretability, we tokenize the dataset at the word level, by ﬁrst converting all characters\\nto lowercase, separating punctuation from words, and splitting on whitespace. Using 10 million\\nrandomly chosen sentences from the dataset, we build a vocabulary consisting of the 30,000 most\\ncommonly occurring words. We ﬁlter sentences which are longer than 15 tokens.\\nA.4 Recurrent Neural Networks\\nThe three types of RNNs we use in this work are speciﬁed below. Wandbrepresent trainable\\nweight matrices and bias parameters, respectively, and htdenotes the hidden state at timestep t'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 15}, page_content='The three types of RNNs we use in this work are speciﬁed below. Wandbrepresent trainable\\nweight matrices and bias parameters, respectively, and htdenotes the hidden state at timestep t\\n(representing either the encoder or decoder). All other vectors ( ct,gt,rt,it,ft) represent intermediate\\nquantities at time step t;σ(·)represents a pointwise sigmoid nonlinearity; and f(·)is the pointwise\\ntanh nonlinearity.\\nGated Recurrent Unit (GRU) The hidden state update expression for the GRU Cho et al. (2014)\\nis given by\\nht=gt·ht−1+ (1−gt)·ct, (18a)\\nwith\\nct=f(\\nWch(r·ht−1) +Wcxxt+bc)\\n, (18b)\\ngt=σ(\\nWghht−1+Wgxxt+bg)\\n, (18c)\\nrt=σ(\\nWrhht−1+Wrxxt+br)\\n, (18d)\\nUpdate-Gate RNN (UGRNN) The hidden state update expression for the UGRNN Collins et al.\\n(2016) is given by\\nht=gt·ht−1+ (1−gt)·ct, (19a)\\nwith\\nct=f(\\nWchht−1+Wcxxt+bc)\\n, (19b)\\ngt=σ(\\nWghht−1+Wgxxt+bg)\\n, (19c)\\nLong-Short-Term-Memory (LSTM) Unlike the GRU and the UGRNN, the LSTM Hochreiter &'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 15}, page_content='(2016) is given by\\nht=gt·ht−1+ (1−gt)·ct, (19a)\\nwith\\nct=f(\\nWchht−1+Wcxxt+bc)\\n, (19b)\\ngt=σ(\\nWghht−1+Wgxxt+bg)\\n, (19c)\\nLong-Short-Term-Memory (LSTM) Unlike the GRU and the UGRNN, the LSTM Hochreiter &\\nSchmidhuber (1997) transfers both a “hidden state” and a cell state from one time step to the next. In\\norder to cast the LSTM update expressions into the same form as the GRU and UGRNN, we deﬁne\\nits hidden state to be\\nht=[\\nct,˜ht]\\n, (20a)\\nwith the update expression given by\\n˜ht=f(ct)·σ(\\nWhhht−1+Whxxt+bh)\\n, (20b)\\nct=ft·ct−1+i·σ(\\nWch˜ht−1+Wcxxt+bc)\\n, (20c)\\nit=σ(\\nWihht−1+Wixxt+bi)\\n, (20d)\\nft=σ(\\nWfhht−1+Wfxxt+bf)\\n. (20e)\\n16'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 16}, page_content='For the LSTM, we only use ˜hE\\ntand˜hD\\nsfor the determination of the context vector and as the decoder\\noutput that is passed to the readout. That is, for AED, ys=W[˜hD\\ns,cs] +bwithcsthe same as (5)\\nwith all hE\\nt→˜hE\\ntand the alignment ast:=˜hD\\ns·˜hE\\nt.\\nA.5 Training\\nFor the one-to-one and eSCAN datasets, we train networks with the ADAM optimizer (Kingma & Ba,\\n2014) and an exponentially-decaying learning rate schedule with an initial learning rate of η= 0.1\\nand a decay rate of 0.9997 every step (with the exception of the VED networks, in which case we\\nused a decay rate of 0.9999 ). Cross-entropy loss with ℓ2regularization was used and gradients were\\nclipped at a maximum value of 10. Both datasets used a batch size of 128and each dynamically\\ngenerated dataset was trained over two epochs. For the AED and VED architectures, a hidden\\ndimension size of n= 128 was used, while for the AO we used n= 256 (for LSTM cells, both the'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 16}, page_content='generated dataset was trained over two epochs. For the AED and VED architectures, a hidden\\ndimension size of n= 128 was used, while for the AO we used n= 256 (for LSTM cells, both the\\nhidden-state˜htand the memory ctaren-dimensional). For these synthetic experiments, we do not\\nadd a bias term to this linear readout layer for the purposes of simplicity and ease of interpretation.\\nAll training for these tasks was performed on GPUs and took at most 20 minutes.\\nAs mentioned above, due to the small vocabulary size of one-to-one and eSCAN, we simply pass\\none-hot encoded inputs in the RNN architectures, i.e. we use no embedding layer. For the AO\\narchitecture, the input dimension was padded up 50and100for the one-to-one and eSCAN tasks,\\nrespectively. Positional encoding vectors were rotated by a random orthonormal matrix so they\\nwere misaligned with the one-hot-encoded input vectors. Finally, we found performance improved'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 16}, page_content='respectively. Positional encoding vectors were rotated by a random orthonormal matrix so they\\nwere misaligned with the one-hot-encoded input vectors. Finally, we found performance improved\\nwhen positional encoding time-scale τwas chosen to be of order the phrase length: τ= 50 for the\\none-to-one tasks and τ= 100 for the eSCAN tasks.\\nFor the natural translation datasets, each token was mapped to an embedded vector of dimension\\n128 using a learned embedding layer. Both the AED and AO architectures used GRU cells, with\\nhidden dimension size of n= 128 . As in the synthetic datasets, we train using the ADAM optimizer\\nfor 30,000 steps using a batch size of 64; we use an exponential learning rate schedule with initial\\nη= 0.01and a decay rate of 0.99995. Gradients are clipped to a maximum value of 30.\\nB Additional Results\\nIn this section, we discuss several additional results that supplement those discussed in the main text.\\nB.1 LSTM and UGRNN Cells'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 16}, page_content='B Additional Results\\nIn this section, we discuss several additional results that supplement those discussed in the main text.\\nB.1 LSTM and UGRNN Cells\\nIn the main text, all plots used a GRU RNN for the cells in the encoder and decoder. For the UGRNN\\nand LSTM we ﬁnd qualitatively similar results on the one-to-one task. Summaries of our results for\\nthe two RNN cells are shown in Fig. 8 and Fig. 9, respectively. For all types of cells, the networks\\nachieve 100% test accuracy on the one-to-one task.\\nNotably, for the AED architecture, we observe that the LSTM has a slightly different attention matrix\\nthan that of the GRU and UGRNN. Namely, for the ﬁrst few decoder time steps, the LSTM appears\\nto attend to hE\\nTof a given phrase. This means that the network is transferring information about\\nthe ﬁrst few inputs all the way to the ﬁnal encoder hidden state. As such, it appears that the AED\\narchitecture with LSTM cells relies more on recurrence to solve the one-to-one task relative to its'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 16}, page_content='the ﬁrst few inputs all the way to the ﬁnal encoder hidden state. As such, it appears that the AED\\narchitecture with LSTM cells relies more on recurrence to solve the one-to-one task relative to its\\nGRU and UGRNN counterparts. Similar to our ﬁndings for the VED architecture in the main text,\\nwe ﬁnd the input-delta components to be signiﬁcantly less clustered around their corresponding\\ninput components when this occurs (Fig. 9b). In contrast, the AO architecture with LSTM cells is\\nqualitatively similar to that with the GRU or UGRNN cells.\\nWe also train the AO architecture with LSTM and UGRNN cells on eSCAN. We again see qualitatively\\nsimilar behavior to what we saw in the main text for AO with GRU cells (Fig. 10). For both types\\nof cells, we see the temporal components align to form an approximately diagonal attention matrix\\n(Fig. 10b,e). Once again, we see the input-delta components to be closely clustered around their'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 16}, page_content='of cells, we see the temporal components align to form an approximately diagonal attention matrix\\n(Fig. 10b,e). Once again, we see the input-delta components to be closely clustered around their\\ncorresponding input components, and said input components are close to their respective readouts,\\nimplementing the translation dictionary (Fig. 10c,f).\\n17'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 17}, page_content='Figure 8: Summary of dynamics for all three architectures on the one-to-one task with UGRNN cells.\\nAll three architectures trained on an N= 3one-to-one task of variable length ranging from 15to20.(a)For\\nAED, the path formed by the temporal components of the encoder (orange) and decoder (purple), µE\\ntandµD\\ns.\\nWe denote the ﬁrst and last temporal component by a square and star, respectively, and the color of said path is\\nlighter for earlier times. The inset shows the softmaxed alignment scores for µD\\ns·µE\\nt, which we ﬁnd to be a\\ngood approximation to the full alignment for the one-to-one task. (b)The input-delta components of the encoder\\n(light) and decoder (dark) colored by word (see labels). The encoder input components, χE\\nxare represented by a\\ndark colored ‘X’. The solid lines are the readout vectors (see labels on (d)). (c, d) The same plots for the AO\\nnetwork. (e, f) The same plots for the VED network (with no attention inset).'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 17}, page_content='dark colored ‘X’. The solid lines are the readout vectors (see labels on (d)). (c, d) The same plots for the AO\\nnetwork. (e, f) The same plots for the VED network (with no attention inset).\\nFigure 9: Summary of dynamics for all three architectures on the one-to-one translation task with LSTM\\ncells. All three architectures trained on an N= 3 one-to-one translation task with inputs of variable length\\nranging from 15to20.(a)For AED, the path formed by the temporal components of the encoder (orange) and\\ndecoder (purple), µE\\ntandµD\\ns. We denote the ﬁrst and last temporal component by a square and star, respectively,\\nand the color of said path is lighter for earlier times. The inset shows the softmaxed alignment scores for\\nµD\\ns·µE\\nt, which we ﬁnd to be a good approximation to the full alignment for the one-to-one task. (b)The\\ninput-delta components of the encoder (light) and decoder (dark) colored by word (see labels). The encoder\\ninput components, χE'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 17}, page_content='input-delta components of the encoder (light) and decoder (dark) colored by word (see labels). The encoder\\ninput components, χE\\nxare represented by a dark colored ‘X’. The solid lines are the readout vectors (see labels\\non (d)). (c, d) The same plots for the AO network. (e, f) The same plots for the VED network (with no attention\\ninset).\\n18'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 18}, page_content='B.2 Autonomous Dynamics and Temporal Components\\nWhen the temporal components are the leading-order behavior in alignment scores (e.g. in the\\none-to-one and eSCAN tasks), we ﬁnd them to be well approximated the hidden states resulting from\\nthe network having zero input, i.e. xE\\nt=0for allt. In the case of AED and VED, this means the\\nencoder RNNs are driven solely by their recurrent behavior. For AO, the encoder RNNs are driven by\\nonly the positional encoding vectors. Since in all three cases this results network outputting hidden\\nstates independent of the details of the input, we call this the autonomous dynamics of the network.\\nDenote the encoder and decoder hidden states resulting from no input by hE,0\\ntandhD,0\\ns, respectively.\\nFor AED and VED, they are\\nhE,0\\nt=FE(hE,0\\nt−1,0), hD,0\\ns=FD(hD,0\\ns−1,0). (21)\\nFor the AO network, we still add the positional encoding vectors to the inputs,\\nhE,0\\nt=FE(0,pE\\nt), hD,0\\ns=FD(0,pD\\ns). (22)'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 18}, page_content='For AED and VED, they are\\nhE,0\\nt=FE(hE,0\\nt−1,0), hD,0\\ns=FD(hD,0\\ns−1,0). (21)\\nFor the AO network, we still add the positional encoding vectors to the inputs,\\nhE,0\\nt=FE(0,pE\\nt), hD,0\\ns=FD(0,pD\\ns). (22)\\nPlotting the resulting hidden states along with the temporal components, we ﬁnd for all three\\narchitectures the two quantities are quite close at all time steps of the encoder and decoder (Fig. 11).\\nWe quantify the degree to which the null hidden states approximate the temporal components via\\n∥hE\\nt−hE,0\\nt∥2/∥hE\\nt∥2and∥hD\\ns−hD,0\\ns∥2/∥hD\\ns∥2where∥·∥2denotes theℓ2-norm. For the AO network,\\nwe ﬁnd the average of this quantity across the entire encoder and decoder phrases to be about 0.07\\nand0.08, respectively, while for AED we ﬁnd it to be 0.07for the encoder and 0.19for the decoder.\\nWe also ﬁnd the null hidden states to be close to the temporal components of eSCAN (Fig. 12). Again,\\naveraging our difference measure across the entire encoder and decoder phrases, for AO we ﬁnd 0.11'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 18}, page_content='We also ﬁnd the null hidden states to be close to the temporal components of eSCAN (Fig. 12). Again,\\naveraging our difference measure across the entire encoder and decoder phrases, for AO we ﬁnd 0.11\\nand0.15and for AED 0.21and0.17, respectively.\\nThis result gives insight into the network dynamics that drive the behavior of the temporal components.\\nIn AED and VED, each RNN cell is driven by two factors: the recurrent hidden state and the input.\\nThe AO architecture is similar, but the recurrence is replaced by temporal information through the\\npositional encoding. The absence of input eliminates the input-driven behavior in the RNN cells.\\nSince the network’s hidden states still trace out paths very close to the temporal components, this is\\nevidence that it is the recurrent dynamics (in the case of AED and VED) or the positional encoding\\nvectors that drive the network’s temporal component behavior.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 18}, page_content='evidence that it is the recurrent dynamics (in the case of AED and VED) or the positional encoding\\nvectors that drive the network’s temporal component behavior.\\nOne can use this information to postulate other behaviors in the network. For instance, given the lack\\nof correlation between inputs in the one-to-one translation task, in the AO network one may wonder\\nhow much of the decoder’s dynamics are driven by recurrent versus input behavior. We already know\\nthe decoder’s primary job in this network is to align its temporal components with that of the encoder,\\nand the results above suggest said behavior is driven primarily by the positional encoding vectors and\\nnot the inputs. To test this, we compare the accuracies of a trained network with and without inputs\\ninto the decoder RNN. We still achieve 100% word accuracy when xD\\ns=0for all decoder time steps.\\nB.3 Learned Attention\\nIn addition to the dot-product attention considered throughout the main text, we have implemented'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 18}, page_content='s=0for all decoder time steps.\\nB.3 Learned Attention\\nIn addition to the dot-product attention considered throughout the main text, we have implemented\\nand analyzed new architectures that are identical to the AED and AO architectures that use a learned-\\nattention mechanism. These networks use a scaled-dot product attention in the form of queries, keys,\\nand value matrices similar to the original Transformer Vaswani et al. (2017). More speciﬁcally, the\\ncontext vector csand alignment astare now determined by the expressions\\ncs:=T∑\\nt=1αstvt, ast:=qs·vt. (23)\\nIn these expressions, the vectors vt,qs, andktare product of the learned weight matrices V∈Rn×n,\\nQ∈Rn′×n, andK∈Rn′×nand the hidden states,\\nvt:=VhE\\nt, qs:=QhD\\ns, kt:=KhE\\nt, (24)\\nwithvt∈Rn(i.e. the same dimension as hidden state space) and qs,kt∈Rn′, withn′the dimension\\nof the query/key space.\\n19'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 19}, page_content='Figure 10: Summary of dynamics for AO architectures with LSTM and UGRNN cells trained on eSCAN.\\n(a)Example attention matrix for the AED architecture. (b)AED network’s temporal components, with the inset\\nshowing the attention matrix from said temporal components. Once again, encoder and decoder components are\\norange and purple, respectively. (c)AED network’s input-delta components, input components, and readouts, all\\ncolored by their corresponding input/output words (see labels). (d, e, f) The same plots for UGRNN cells.\\nFigure 11: Autonomous dynamics versus temporal components for architectures trained on one-to-one\\ntranslation. All three architectures are trained on an N= 3one-to-one translation task with inputs of variable\\nlength ranging from 15to20.(a)For AED, the path formed by the temporal components of the encoder (orange),\\nµE\\nt. Also plotted in green are the null hidden states, hE,0\\nt. We denote the ﬁrst and last hidden state of each of'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 19}, page_content='µE\\nt. Also plotted in green are the null hidden states, hE,0\\nt. We denote the ﬁrst and last hidden state of each of\\nthese by a square and star, respectively. The inset shows the quantitative difference between the two states,\\n∥hE\\nt−hE,0\\nt∥2/∥hE\\nt∥2, as a function of encoder time step, t.(b)The same plot but for the decoder temporal\\ncomponents (purple) and the decoder null hidden states (red). (c, d) The same plots for the AO network. (e, f)\\nThe same plots for the VED network.\\n20'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 20}, page_content='Figure 12: Autonomous dynamics versus temporal components for architectures trained on eSCAN. (a)\\nFor AED, the path formed by the temporal components of the encoder (orange), µE\\nt. Also plotted in green\\nare the null hidden states, hE,0\\nt. We denote the ﬁrst and last hidden state of each of these by a square and star,\\nrespectively. The inset shows the quantitative difference between the two states, ∥hE\\nt−hE,0\\nt∥2/∥hE\\nt∥2, as a\\nfunction of encoder time step, t.(b)The same plot but for the decoder temporal components (purple) and the\\ndecoder null hidden states (red). (c, d) The same plots for the AO network.\\nAfter training these networks on our one-to-one and eSCAN tasks, we ﬁnd very similar results to that\\nof dot-product attention. In particular, temporal components of the encoder and decoder continue to\\nalign with one another after being projected through the respective key/query matrix (for AO, see'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 20}, page_content='of dot-product attention. In particular, temporal components of the encoder and decoder continue to\\nalign with one another after being projected through the respective key/query matrix (for AO, see\\nFig. 13a). Input-delta components cluster along the respective readouts, after being projected through\\nthe value matrix (Fig. 13b). Decomposing the alignment scores for these networks, we continue to\\nﬁnd them to be dominated by the temporal component term (i.e. µD\\nsQTKµE\\nt).\\nThis gives us additional conﬁdence that the analysis techniques can be applied to modern architectures\\nthat use attention. For instance, in a multi-headed attention setting, such a decomposition could be\\nused on each head separately to characterize the dynamics behind each of the heads.\\nB.4 Attention Only with Non-Gated Feedforward\\nThe AO architecture’s feedfoward networks are created by zeroing the recurrent part of various RNNs.'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 20}, page_content='B.4 Attention Only with Non-Gated Feedforward\\nThe AO architecture’s feedfoward networks are created by zeroing the recurrent part of various RNNs.\\nAlthough this does result in a feedforward network, the presence of the gating mechanisms in the\\nRNN cells we use in this work make this non-standard feedforward network. To verify our qualitative\\nresults hold beyond a gated feedfoward network, we investigated if our results differed when using a\\nstandard fully connected layer followed by a tanh readout.\\nWe ﬁnd the non-gated AO network trains well on both the one-to-one and eSCAN tasks, achieving\\n100% and 98.8% word accuracy, respectively. Again performing the temporal and input component\\ndecomposition on the network’s hidden states, we ﬁnd the qualitative dynamics of this network are\\nthe same as its RNN counterparts (Fig. 14). For example, in the one-to-one translation task we ﬁnd\\nthe temporal components of the encoder and decoder again mirror one another in order to from a'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 20}, page_content='the same as its RNN counterparts (Fig. 14). For example, in the one-to-one translation task we ﬁnd\\nthe temporal components of the encoder and decoder again mirror one another in order to from a\\ndiagonal attention matrix (Fig. 14a). The input components of the encoder align with the readouts to\\nimplement the translation dictionary (Fig. 14b).\\n21'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 21}, page_content='Figure 13: Dynamics for AO with learned attention. (a) The path formed by the temporal components of\\nthe encoder (orange) and decoder (purple) multiplied by the key and query matrices, respectively, i.e. KµE\\ntand\\nQµD\\ns. We denote the ﬁrst and last temporal component by a square and star, respectively, and the color of said\\npath is lighter for earlier times. The inset shows the softmaxed alignment scores for qs·kt, which we ﬁnd to be\\na good approximation to the full alignment for the one-to-one translation task. (b)The input-delta components\\nof the encoder (light) and decoder (dark) colored by word (see labels), after being multiplied by value matrix,\\nV. The encoder input components, VχE\\nxare represented by a dark colored ‘X’. The solid lines are the readout\\nvectors.\\nB.5 Encoder-Decoder with Attention Readouts\\nThe AED network’s linear readout takes into account both the decoder hidden state output and the\\ncontext vector, i.e. ys=W[hD'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 21}, page_content='vectors.\\nB.5 Encoder-Decoder with Attention Readouts\\nThe AED network’s linear readout takes into account both the decoder hidden state output and the\\ncontext vector, i.e. ys=W[hD\\ns,cs]. As such, each output’s readout can be viewed as twovectors\\nin hidden state space: one which acts on the context vector and the other which acts on the decoder\\nhidden state output. Here we elaborate on our comment in the main text regarding the effects of the\\ndecoder readouts being negligible.\\nRecall that after training the AED network, we found the context vector readouts align with the\\nencoder input components, yielding the translation dictionary for a given task. We ﬁnd the the decoder\\nreadouts to be close to orthogonal to the context vector readouts (Fig. 15). Furthermore, we ﬁnd the\\nreadouts for all words other than the ‘eos’ character to be closely aligned. Omitting the ‘eos‘ effect,\\nthis results in the decoder readouts contributing roughly equal values to all logit values. Since the'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 21}, page_content='this results in the decoder readouts contributing roughly equal values to all logit values. Since the\\nlogit values are passed through a softmax function, it is their differences that ultimately matter when\\nit comes to classifying a given output as a word. In contrast, we found the context vector readouts to\\nalign with the vertices of an (N−1)-simplex and the logit value contributions to vary signiﬁcantly\\nmore with the hidden state. Indeed, comparing the difference between the largest and second largest\\nlogit contribution of each set of readouts, we ﬁnd the difference due to the context vector readouts to\\nbe several times larger than that of the decoder readouts. For AED trained on eSCAN, we again ﬁnd\\nthe differences in the context vector logit contributions to be several times larger than that due to the\\ndecoder hidden states.\\nB.6 Vanilla Encoder-Decoder Dynamics\\nIn the main text, we brieﬂy discussed the dynamics of the VED architecture, and here we provide'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 21}, page_content='decoder hidden states.\\nB.6 Vanilla Encoder-Decoder Dynamics\\nIn the main text, we brieﬂy discussed the dynamics of the VED architecture, and here we provide\\nsome additional details. After training the VED architecture on the one-to-one translation task, we\\nfound that the encoder and decoder hidden states belonging to the same time step formed clusters,\\nand said clusters are closest to those corresponding to adjacent time steps. Additionally, since the\\nVED arhcitecture has no attention, the encoder and decoder hidden states have to carry all relevant\\ninformation from preceding steps. To facilitate this, the dynamics of the encoder’s hidden state space\\norganizes itself into a tree structure to encode the input phrases (Fig. 16). Starting from the encoder’s\\ninitial hidden state, the hidden states of a given input phrase traverse the branches of said tree as the\\nphrase is read in, ultimately arriving at one of the tree’s leaves at time T. The distinct leaves represent'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 21}, page_content='phrase is read in, ultimately arriving at one of the tree’s leaves at time T. The distinct leaves represent\\nthe different ﬁnal encoder hidden states, hE\\nT, that must encode the input phrase’s words and their\\nordering.\\nSince the decoder receives no additional input, the encoder must place hE\\nTin a location of hidden\\nstate space for the decoder’s dynamics to produce the entire output phrase. Although the network\\ndoes indeed learn to do this, we do not observe the reversal of the tree structure learned by the\\n22'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 22}, page_content='Figure 14: AO with non-gated feed foward trained on one-to-one and eSCAN. (a) For the AO trained on\\nanN= 3one-to-one translation task, the path formed by the temporal components of the encoder (orange) and\\ndecoder (purple), µE\\ntandµD\\ns. We denote the ﬁrst and last temporal component by a square and star, respectively,\\nand the color of said path is lighter for earlier times. The inset shows the softmaxed alignment scores for µD\\ns·µE\\nt,\\nwhich we ﬁnd to be a good approximation to the full alignment for the one-to-one translation task. (b)The\\ninput-delta components of the encoder (light) and decoder (dark) colored by word (see labels). The encoder\\ninput components, χE\\nxare represented by a dark colored ‘X’. The solid lines are the readout vectors (see labels).\\n(c, d) The same plots for AO trained on eSCAN.\\nencoder. That is, any two phrases that have the same output sequence for any time s≥s′could'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 22}, page_content='(c, d) The same plots for AO trained on eSCAN.\\nencoder. That is, any two phrases that have the same output sequence for any time s≥s′could\\noccupy the same decoder hidden states ˜hsfors≥s′. This would result in the temporal mirror of the\\ntree structure seen in the encoder. However, such a structure is not observed and the decoder instead\\nseems to arrive at a solution where all output paths are kept distinct.\\n23'),\n",
       " Document(metadata={'source': 'encoder-decoder.pdf', 'page': 23}, page_content='Figure 15: Decoder hidden state behavior in AED trained on one-to-one translation. The input-delta\\ncomponents of the encoder (light) and decoder (dark) colored by word. The encoder input components, χE\\nx\\nare represented by a dark colored ‘X’. The solid lines are the readout vectors, with those corresponding to the\\ncontext vector colored dark (and labeled by ‘RO’) and those corresponding to the decoder hidden state readout\\ncolored light (and labeled by ‘Dec RO’).\\nFigure 16: Tree-like structure of encoder hidden states in VED. The encoder hidden states of the VED\\narchitecture trained on the one-to-one translation task for the ﬁrst ﬁve time steps. Note the hidden states organize\\nthemselves in ﬁve distinct clusters along PC 0. Also shown are the paths in hidden state space various input\\nphrases take as a given input phrase is read. Said paths are colored from red to black by similarity starting from\\nlatest time, i.e.{B,A,C,A,C}and{B,A,C,A,B}are similarly colored but {C,A,C,A,C}is not.\\n24')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "chunk_docs = splitter.split_documents(docs)\n",
    "chunk_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding technique steps 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_17156\\85972367.py:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  db = FAISS.from_documents(chunk_docs,OpenAIEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "## FAISS - Facebook AI similarity search\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(chunk_docs,OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x181eb916790>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.vectorstores.faiss.FAISS"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"We develop a method for decomposing the hidden states of the network into a sum of components that let us isolate input driven behavior from temporal (or sequence) driven behavior.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Contributions\n",
      "•We propose a decomposition of hidden state dynamics into separate pieces, one of which explains\n",
      "the temporal behavior of the network, another of which describes the input behavior. We show\n",
      "such a decomposition aids in understanding the behavior of networks with attention.\n",
      "•In the tasks studied, we show the temporal (input) components play a larger role in determining\n",
      "the attention matrix as the average attention matrix becomes a better (worse) approximation for a\n",
      "random sample’s attention matrix.\n",
      "•We discuss the dynamics of architectures with attention and/or recurrence and show how the\n",
      "input/temporal component behavior differs across said architectures.\n",
      "•We investigate the detailed temporal and input component dynamics in a synthetic setting to\n",
      "understand the mechanism behind common sequence-to-sequence structures and how they might\n",
      "differ in the presence of recurrence.\n",
      "Related Work As mentioned in the introduction, a common technique to gain some understanding\n"
     ]
    }
   ],
   "source": [
    "retrieve_result = db.similarity_search(query)\n",
    "print(retrieve_result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
